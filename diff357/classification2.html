
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.17.1: http://docutils.sourceforge.net/" name="generator"/>
<title>6. Classification II: evaluation &amp; tuning — Data Science: A First Introduction with Python</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet"/>
<link href="_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet"/>
<link href="_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet"/>
<link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet"/>
<link as="font" crossorigin="" href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" rel="stylesheet" type="text/css"/>
<link href="_static/togglebutton.css" rel="stylesheet" type="text/css"/>
<link href="_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css"/>
<link href="_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="_static/style.css" rel="stylesheet" type="text/css"/>
<link href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" rel="preload"/>
<link as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" rel="preload"/>
<script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>
<script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
<script src="_static/jquery.js"></script>
<script src="_static/underscore.js"></script>
<script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="_static/doctools.js"></script>
<script src="_static/clipboard.min.js"></script>
<script src="_static/copybutton.js"></script>
<script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="_static/togglebutton.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="_static/design-tabs.js"></script>
<script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-7XBFF4RSN2"></script>
<script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-7XBFF4RSN2');
            </script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="_static/sphinx-thebe.js"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'classification2';</script>
<link href="genindex.html" rel="index" title="Index"/>
<link href="search.html" rel="search" title="Search"/>
<link href="regression1.html" rel="next" title="7. Regression I: K-nearest neighbors"/>
<link href="classification1.html" rel="prev" title="5. Classification I: training &amp; predicting"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
<script src="website_diff.js"></script><link href="website_diff.css" rel="stylesheet" type="text/css"/></head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<a class="skip-link" href="#main-content">Skip to main content</a>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<nav class="bd-header navbar navbar-expand-lg bd-navbar">
</nav>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="index.html">
<p class="title logo__title">Data Science: A First Introduction with Python</p>
</a></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Front Matter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal link-to-diff" href="preface-text.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="foreword-text.html">Foreword</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgements.html">Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="authors.html">About the authors</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal link-to-diff" href="intro.html">1. Python and Pandas</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="reading.html">2. Reading in data locally and from the web</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="wrangling.html">3. Cleaning and wrangling data</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="viz.html">4. Effective data visualization</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="classification1.html">5. Classification I: training &amp; predicting</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Classification II: evaluation &amp; tuning</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="regression1.html">7. Regression I: K-nearest neighbors</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="regression2.html">8. Regression II: linear regression</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="clustering.html">9. Clustering</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="inference.html">10. Statistical inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="jupyter.html">11. Combining code and text with Jupyter</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="version-control.html">12. Collaboration with version control</a></li>
<li class="toctree-l1"><a class="reference internal link-to-diff" href="setup.html">13. Setting up your computer</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-bars"></span>
</label></div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<div class="dropdown dropdown-download-buttons">
<button aria-expanded="false" aria-label="Download this page" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fas fa-download"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm btn-download-source-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="_sources/classification2.md" target="_blank" title="Download source file">
<span class="btn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="btn__text-container">.md</span>
</a>
</li>
<li>
<button class="btn btn-sm btn-download-pdf-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" onclick="window.print()" title="Print to PDF">
<span class="btn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
<button class="btn btn-sm btn-fullscreen-button" data-bs-placement="bottom" data-bs-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="btn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__secondary" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</label>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Classification II: evaluation &amp; tuning</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">6.1. Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-learning-objectives">6.2. Chapter learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-performance">6.3. Evaluating performance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#randomness-and-seeds">6.4. Randomness and seeds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-performance-with-scikit-learn">6.5. Evaluating performance with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-train-test-split">6.5.1. Create the train / test split</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocess-the-data">6.5.2. Preprocess the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-classifier">6.5.3. Train the classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-the-labels-in-the-test-set">6.5.4. Predict the labels in the test set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-performance">6.5.5. Evaluate performance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#critically-analyze-performance">6.5.6. Critically analyze performance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-the-classifier">6.6. Tuning the classifier</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">6.6.1. Cross-validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-value-selection">6.6.2. Parameter value selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#under-overfitting">6.6.3. Under/Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-on-the-test-set">6.6.4. Evaluating on the test set</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">6.7. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-variable-selection">6.8. Predictor variable selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-effect-of-irrelevant-predictors">6.8.1. The effect of irrelevant predictors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-a-good-subset-of-predictors">6.8.2. Finding a good subset of predictors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-selection-in-python">6.8.3. Forward selection in Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">6.9. Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">6.10. Additional resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">6.11. References</a></li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" role="main">
<section class="tex2jax_ignore mathjax_ignore" id="classification-ii-evaluation-tuning">
<span id="classification2"></span><h1><span class="section-number">6. </span>Classification II: evaluation &amp; tuning<a class="headerlink" href="#classification-ii-evaluation-tuning" title="Permalink to this heading">#</a></h1>
<section id="overview">
<h2><span class="section-number">6.1. </span>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h2>
<p>This chapter continues the introduction to predictive modeling through
classification. While the previous chapter covered training and data
preprocessing, this chapter focuses on how to evaluate the performance of
a classifier, as well as how to improve the classifier (where possible)
to maximize its accuracy.</p>
</section>
<section id="chapter-learning-objectives">
<h2><span class="section-number">6.2. </span>Chapter learning objectives<a class="headerlink" href="#chapter-learning-objectives" title="Permalink to this heading">#</a></h2>
<p>By the end of the chapter, readers will be able to do the following:</p>
<ul class="simple">
<li><p>Describe what training, validation, and test data sets are and how they are used in classification.</p></li>
<li><p>Split data into training, validation, and test data sets.</p></li>
<li><p>Describe what a random seed is and its importance in reproducible data analysis.</p></li>
<li><p>Set the random seed in Python using the <code class="docutils literal notranslate"><span class="pre">numpy.random.seed</span></code> function.</p></li>
<li><p>Describe and interpret accuracy, precision, recall, and confusion matrices.</p></li>
<li><p>Evaluate classification accuracy, precision, and recall in Python using a test set, a single validation set, and cross-validation.</p></li>
<li><p>Produce a confusion matrix in Python.</p></li>
<li><p>Choose the number of neighbors in a K-nearest neighbors classifier by maximizing estimated cross-validation accuracy.</p></li>
<li><p>Describe underfitting and overfitting, and relate it to the number of neighbors in K-nearest neighbors classification.</p></li>
<li><p>Describe the advantages and disadvantages of the K-nearest neighbors classification algorithm.</p></li>
</ul>
</section>
<section id="evaluating-performance">
<h2><span class="section-number">6.3. </span>Evaluating performance<a class="headerlink" href="#evaluating-performance" title="Permalink to this heading">#</a></h2>
<p id="index-0">Sometimes our classifier might make the wrong prediction. A classifier does not
need to be right 100% of the time to be useful, though we don’t want the
classifier to make too many wrong predictions. How do we measure how “good” our
classifier is? Let’s revisit the
<a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">breast cancer images data</a> <span id="id1">[<a class="reference internal" href="#id16" title="William Nick Street, William Wolberg, and Olvi Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In International Symposium on Electronic Imaging: Science and Technology. 1993.">Street <em>et al.</em>, 1993</a>]</span>
and think about how our classifier will be used in practice. A biopsy will be
performed on a <em>new</em> patient’s tumor, the resulting image will be analyzed,
and the classifier will be asked to decide whether the tumor is benign or
malignant. The key word here is <em>new</em>: our classifier is “good” if it provides
accurate predictions on data <em>not seen during training</em>, as this implies that
it has actually learned about the relationship between the predictor variables and response variable,
as opposed to simply memorizing the labels of individual training data examples.
But then, how can we evaluate our classifier without visiting the hospital to collect more
tumor images?</p>
<p id="index-1">The trick is to split the data into a <strong>training set</strong> and <strong>test set</strong> (<a class="reference internal" href="#fig-06-training-test"><span class="std std-numref">Fig. 6.1</span></a>)
and use only the <strong>training set</strong> when building the classifier.
Then, to evaluate the performance of the classifier, we first set aside the labels from the <strong>test set</strong>,
and then use the classifier to predict the labels in the <strong>test set</strong>. If our predictions match the actual
labels for the observations in the <strong>test set</strong>, then we have some
confidence that our classifier might also accurately predict the class
labels for new observations without known class labels.</p>
<div class="admonition note" id="index-2">
<p class="admonition-title">Note</p>
<p>If there were a golden rule of machine learning, it might be this:
<em>you cannot use the test data to build the model!</em> If you do, the model gets to
“see” the test data in advance, making it look more accurate than it really
is. Imagine how bad it would be to overestimate your classifier’s accuracy
when predicting whether a patient’s tumor is malignant or benign!</p>
</div>
<figure class="align-default" id="fig-06-training-test">
<img alt="_images/training_test.png" src="_images/training_test.png"/>
<figcaption>
<p><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Splitting the data into training and testing sets.</span><a class="headerlink" href="#fig-06-training-test" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<span class="target" id="index-3"></span><p id="index-4">How exactly can we assess how well our predictions match the actual labels for
the observations in the test set? One way we can do this is to calculate the
prediction <strong>accuracy</strong>. This is the fraction of examples for which the
classifier made the correct prediction. To calculate this, we divide the number
of correct predictions by the number of predictions made.
The process for assessing if our predictions match the actual labels in the
test set is illustrated in <a class="reference internal" href="#fig-06-ml-paradigm-test"><span class="std std-numref">Fig. 6.2</span></a>.</p>
<div class="math notranslate nohighlight">
\[\mathrm{accuracy} = \frac{\mathrm{number \; of  \; correct  \; predictions}}{\mathrm{total \;  number \;  of  \; predictions}}\]</div>
<figure class="align-default" id="fig-06-ml-paradigm-test">
<img alt="_images/ML-paradigm-test.png" src="_images/ML-paradigm-test.png"/>
<figcaption>
<p><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Process for splitting the data and finding the prediction accuracy.</span><a class="headerlink" href="#fig-06-ml-paradigm-test" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p id="index-5">Accuracy is a convenient, general-purpose way to summarize the performance of a classifier with
a single number.  But prediction accuracy by itself does not tell the whole
story.  In particular, accuracy alone only tells us how often the classifier
makes mistakes in general, but does not tell us anything about the <em>kinds</em> of
mistakes the classifier makes.  A more comprehensive view of performance can be
obtained by additionally examining the <strong>confusion matrix</strong>. The confusion
matrix shows how many test set labels of each type are predicted correctly and
incorrectly, which gives us more detail about the kinds of mistakes the
classifier tends to make.  <a class="reference internal" href="#confusion-matrix-table"><span class="std std-numref">Table 6.1</span></a> shows an example
of what a confusion matrix might look like for the tumor image data with
a test set of 65 observations.</p>
<table class="table" id="confusion-matrix-table">
<caption><span class="caption-number">Table 6.1 </span><span class="caption-text">An example confusion matrix for the tumor image data.</span><a class="headerlink" href="#confusion-matrix-table" title="Permalink to this table">#</a></caption>
<colgroup>
<col style="width: 33%"/>
<col style="width: 33%"/>
<col style="width: 33%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Predicted Malignant</p></th>
<th class="head"><p>Predicted Benign</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Actually Malignant</strong></p></td>
<td><p>1</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Actually Benign</strong></p></td>
<td><p>4</p></td>
<td><p>57</p></td>
</tr>
</tbody>
</table>
<p>In the example in <a class="reference internal" href="#confusion-matrix-table"><span class="std std-numref">Table 6.1</span></a>, we see that there was
1 malignant observation that was correctly classified as malignant (top left corner),
and 57 benign observations that were correctly classified as benign (bottom right corner).
However, we can also see that the classifier made some mistakes:
it classified 3 malignant observations as benign, and 4 benign observations as
malignant. The accuracy of this classifier is roughly
89%, given by the formula</p>
<div class="math notranslate nohighlight">
\[\mathrm{accuracy} = \frac{\mathrm{number \; of  \; correct  \; predictions}}{\mathrm{total \;  number \;  of  \; predictions}} = \frac{1+57}{1+57+4+3} = 0.892.\]</div>
<p>But we can also see that the classifier only identified 1 out of 4 total malignant
tumors; in other words, it misclassified 75% of the malignant cases present in the
data set! In this example, misclassifying a malignant tumor is a potentially
disastrous error, since it may lead to a patient who requires treatment not receiving it.
Since we are particularly interested in identifying malignant cases, this
classifier would likely be unacceptable even with an accuracy of 89%.</p>
<p id="index-6">Focusing more on one label than the other is
common in classification problems. In such cases, we typically refer to the label we are more
interested in identifying as the <em>positive</em> label, and the other as the
<em>negative</em> label. In the tumor example, we would refer to malignant
observations as <em>positive</em>, and benign observations as <em>negative</em>.  We can then
use the following terms to talk about the four kinds of prediction that the
classifier can make, corresponding to the four entries in the confusion matrix:</p>
<ul class="simple">
<li><p><strong>True Positive:</strong> A malignant observation that was classified as malignant (top left in <a class="reference internal" href="#confusion-matrix-table"><span class="std std-numref">Table 6.1</span></a>).</p></li>
<li><p><strong>False Positive:</strong> A benign observation that was classified as malignant (bottom left in <a class="reference internal" href="#confusion-matrix-table"><span class="std std-numref">Table 6.1</span></a>).</p></li>
<li><p><strong>True Negative:</strong> A benign observation that was classified as benign (bottom right in <a class="reference internal" href="#confusion-matrix-table"><span class="std std-numref">Table 6.1</span></a>).</p></li>
<li><p><strong>False Negative:</strong> A malignant observation that was classified as benign (top right in <a class="reference internal" href="#confusion-matrix-table"><span class="std std-numref">Table 6.1</span></a>).</p></li>
</ul>
<p id="index-7">A perfect classifier would have zero false negatives and false positives (and
therefore, 100% accuracy). However, classifiers in practice will almost always
make some errors. So you should think about which kinds of error are most
important in your application, and use the confusion matrix to quantify and
report them. Two commonly used metrics that we can compute using the confusion
matrix are the <strong>precision</strong> and <strong>recall</strong> of the classifier. These are often
reported together with accuracy.  <em>Precision</em> quantifies how many of the
positive predictions the classifier made were actually positive. Intuitively,
we would like a classifier to have a <em>high</em> precision: for a classifier with
high precision, if the classifier reports that a new observation is positive,
we can trust that the new observation is indeed positive. We can compute the
precision of a classifier using the entries in the confusion matrix, with the
formula</p>
<div class="math notranslate nohighlight">
\[\mathrm{precision} = \frac{\mathrm{number \; of  \; correct \; positive \; predictions}}{\mathrm{total \;  number \;  of \; positive  \; predictions}}.\]</div>
<p><em>Recall</em> quantifies how many of the positive observations in the test set were
identified as positive. Intuitively, we would like a classifier to have a
<em>high</em> recall: for a classifier with high recall, if there is a positive
observation in the test data, we can trust that the classifier will find it.
We can also compute the recall of the classifier using the entries in the
confusion matrix, with the formula</p>
<div class="math notranslate nohighlight">
\[\mathrm{recall} = \frac{\mathrm{number \; of  \; correct  \; positive \; predictions}}{\mathrm{total \;  number \;  of  \; positive \; test \; set \; observations}}.\]</div>
<p>In the example presented in <a class="reference internal" href="#confusion-matrix-table"><span class="std std-numref">Table 6.1</span></a>, we have that the precision and recall are</p>
<div class="math notranslate nohighlight">
\[\mathrm{precision} = \frac{1}{1+4} = 0.20, \quad \mathrm{recall} = \frac{1}{1+3} = 0.25.\]</div>
<p>So even with an accuracy of 89%, the precision and recall of the classifier
were both relatively low. For this data analysis context, recall is
particularly important: if someone has a malignant tumor, we certainly want to
identify it.  A recall of just 25% would likely be unacceptable!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is difficult to achieve both high precision and high recall at
the same time; models with high precision tend to have low recall and vice
versa.  As an example, we can easily make a classifier that has <em>perfect
recall</em>: just <em>always</em> guess positive! This classifier will of course find
every positive observation in the test set, but it will make lots of false
positive predictions along the way  and have low precision. Similarly, we can
easily make a classifier that has <em>perfect precision</em>: <em>never</em> guess
positive! This classifier will never incorrectly identify an obsevation as
positive, but it will make a lot of false negative predictions along the way.
In fact, this classifier will have 0% recall! Of course, most real
classifiers fall somewhere in between these two extremes. But these examples
serve to show that in settings where one of the classes is of interest (i.e.,
there is a <em>positive</em> label), there is a trade-off between precision and recall that one has to
make when designing a classifier.</p>
</div>
</section>
<section id="randomness-and-seeds">
<span id="randomseeds"></span><h2><span class="section-number">6.4. </span>Randomness and seeds<a class="headerlink" href="#randomness-and-seeds" title="Permalink to this heading">#</a></h2>
<p id="index-8">Beginning in this chapter, our data analyses will often involve the use
of <em>randomness</em>. We use randomness any time we need to make a decision in our
analysis that needs to be fair, unbiased, and not influenced by human input.
For example, in this chapter, we need to split
a data set into a training set and test set to evaluate our classifier. We
certainly do not want to choose how to split
the data ourselves by hand, as we want to avoid accidentally influencing the result
of the evaluation. So instead, we let Python <em>randomly</em> split the data.
In future chapters we will use randomness
in many other ways, e.g., to help us select a small subset of data from a larger data set,
to pick groupings of data, and more.</p>
<span class="target" id="index-9"></span><span class="target" id="index-10"></span><p id="index-11">However, the use of randomness runs counter to one of the main
tenets of good data analysis practice: <em>reproducibility</em>. Recall that a reproducible
analysis produces the same result each time it is run; if we include randomness
in the analysis, would we not get a different result each time?
The trick is that in Python—and other programming languages—randomness
is not actually random! Instead, Python uses a <em>random number generator</em> that
produces a sequence of numbers that
are completely determined by a
<em>seed value</em>. Once you set the seed value, everything after that point may <em>look</em> random,
but is actually totally reproducible. As long as you pick the same seed
value, you get the same result!</p>
<p id="index-12">Let’s use an example to investigate how randomness works in Python. Say we
have a series object containing the integers from 0 to 9. We want
to randomly pick 10 numbers from that list, but we want it to be reproducible.
Before randomly picking the 10 numbers,
we call the <code class="docutils literal notranslate"><span class="pre">seed</span></code> function from the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> package, and pass it any integer as the argument.
Below we use the seed number <code class="docutils literal notranslate"><span class="pre">1</span></code>. At
that point, Python will keep track of the randomness that occurs throughout the code.
For example, we can call the <code class="docutils literal notranslate"><span class="pre">sample</span></code> method
on the series of numbers, passing the argument <code class="docutils literal notranslate"><span class="pre">n=10</span></code> to indicate that we want 10 samples.
The <code class="docutils literal notranslate"><span class="pre">to_list</span></code> method converts the resulting series into a basic Python list to make
the output easier to read.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">nums_0_to_9</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>

<span class="n">random_numbers1</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2, 9, 6, 4, 0, 3, 1, 7, 8, 5]
</pre></div>
</div>
</div>
</div>
<p>You can see that <code class="docutils literal notranslate"><span class="pre">random_numbers1</span></code> is a list of 10 numbers
from 0 to 9 that, from all appearances, looks random. If
we run the <code class="docutils literal notranslate"><span class="pre">sample</span></code> method again,
we will get a fresh batch of 10 numbers that also look random.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_numbers2</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[9, 5, 3, 0, 8, 4, 2, 1, 6, 7]
</pre></div>
</div>
</div>
</div>
<p>If we want to force Python to produce the same sequences of random numbers,
we can simply call the <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code> function with the seed value <code class="docutils literal notranslate"><span class="pre">1</span></code>—the same
as before—and then call the <code class="docutils literal notranslate"><span class="pre">sample</span></code> method again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">random_numbers1_again</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers1_again</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2, 9, 6, 4, 0, 3, 1, 7, 8, 5]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_numbers2_again</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers2_again</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[9, 5, 3, 0, 8, 4, 2, 1, 6, 7]
</pre></div>
</div>
</div>
</div>
<p>Notice that after calling <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code>, we get the same
two sequences of numbers in the same order. <code class="docutils literal notranslate"><span class="pre">random_numbers1</span></code> and <code class="docutils literal notranslate"><span class="pre">random_numbers1_again</span></code>
produce the same sequence of numbers, and the same can be said about <code class="docutils literal notranslate"><span class="pre">random_numbers2</span></code> and
<code class="docutils literal notranslate"><span class="pre">random_numbers2_again</span></code>. And if we choose a different value for the seed—say, 4235—we
obtain a different sequence of random numbers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4235</span><span class="p">)</span>
<span class="n">random_numbers1_different</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers1_different</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[6, 7, 2, 3, 5, 9, 1, 4, 0, 8]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_numbers2_different</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers2_different</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[6, 0, 1, 3, 2, 8, 4, 9, 5, 7]
</pre></div>
</div>
</div>
</div>
<p>In other words, even though the sequences of numbers that Python is generating <em>look</em>
random, they are totally determined when we set a seed value!</p>
<p>So what does this mean for data analysis? Well, <code class="docutils literal notranslate"><span class="pre">sample</span></code> is certainly not the
only place where randomness is used in Python. Many of the functions
that we use in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> and beyond use randomness—some
of them without even telling you about it.  Also note that when Python starts
up, it creates its own seed to use. So if you do not explicitly
call the <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code> function, your results
will likely not be reproducible. Finally, be careful to set the seed <em>only once</em> at
the beginning of a data analysis. Each time you set the seed, you are inserting
your own human input, thereby influencing the analysis. For example, if you use
the <code class="docutils literal notranslate"><span class="pre">sample</span></code> many times throughout your analysis but set the seed each time, the
randomness that Python uses will not look as random as it should.</p>
<p>In summary: if you want your analysis to be reproducible, i.e., produce <em>the same result</em>
each time you run it, make sure to use <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code> exactly once
at the beginning of the analysis. Different argument values
in <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code> will lead to different patterns of randomness, but as long as you pick the same
value your analysis results will be the same. In the remainder of the textbook,
we will set the seed once at the beginning of each chapter.</p>
<span class="target" id="index-13"></span><div class="admonition note" id="index-14">
<p class="admonition-title">Note</p>
<p>When you use <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code>, you are really setting the seed for the <code class="docutils literal notranslate"><span class="pre">numpy</span></code>
package’s <em>default random number generator</em>. Using the global default random
number generator is easier than other methods, but has some potential drawbacks. For example,
other code that you may not notice (e.g., code buried inside some
other package) could potentially <em>also</em> call <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code>, thus modifying
your analysis in an undesirable way. Furthermore, not <em>all</em> functions use
<code class="docutils literal notranslate"><span class="pre">numpy</span></code>’s random number generator; some may use another one entirely.
In that case, setting <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code> may not actually make your whole analysis
reproducible.</p>
<p>In this book, we will generally only use packages that play nicely with <code class="docutils literal notranslate"><span class="pre">numpy</span></code>’s
default random number generator, so we will stick with <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code>.
You can achieve more careful control over randomness in your analysis
by creating a <code class="docutils literal notranslate"><span class="pre">numpy</span></code> <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generator.html"><code class="docutils literal notranslate"><span class="pre">Generator</span></code> object</a>
once at the beginning of your analysis, and passing it to
the <code class="docutils literal notranslate"><span class="pre">random_state</span></code> argument that is available in many <code class="docutils literal notranslate"><span class="pre">pandas</span></code> and <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>
functions. Those functions will then use your <code class="docutils literal notranslate"><span class="pre">Generator</span></code> to generate random numbers instead of
<code class="docutils literal notranslate"><span class="pre">numpy</span></code>’s default generator. For example, we can reproduce our earlier example by using a <code class="docutils literal notranslate"><span class="pre">Generator</span></code>
object with the <code class="docutils literal notranslate"><span class="pre">seed</span></code> value set to 1; we get the same lists of numbers once again.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">Generator</span><span class="p">,</span> <span class="n">PCG64</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">PCG64</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">random_numbers1_third</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers1_third</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>array([2, 9, 6, 4, 0, 3, 1, 7, 8, 5])
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">random_numbers2_third</span> <span class="o">=</span> <span class="n">nums_0_to_9</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">random_numbers2_third</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>array([9, 5, 3, 0, 8, 4, 2, 1, 6, 7])
</pre></div>
</div>
</div>
</section>
<section id="evaluating-performance-with-scikit-learn">
<h2><span class="section-number">6.5. </span>Evaluating performance with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code><a class="headerlink" href="#evaluating-performance-with-scikit-learn" title="Permalink to this heading">#</a></h2>
<p id="index-15">Back to evaluating classifiers now!
In Python, we can use the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> package not only to perform K-nearest neighbors
classification, but also to assess how well our classification worked.
Let’s work through an example of how to use tools from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> to evaluate a classifier
using the breast cancer data set from the previous chapter.
We begin the analysis by loading the packages we require,
reading in the breast cancer data,
and then making a quick scatter plot visualization of
tumor cell concavity versus smoothness colored by diagnosis in <a class="reference internal" href="#fig-06-precode"><span class="std std-numref">Fig. 6.3</span></a>.
You will also notice that we set the random seed using the <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code> function,
as described in <a class="reference internal" href="#randomseeds"><span class="std std-numref">Section 6.4</span></a>.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load packages</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">set_config</span>

<span class="c1"># Output dataframes instead of arrays</span>
<span class="n">set_config</span><span class="p">(</span><span class="n">transform_output</span><span class="o">=</span><span class="s2">"pandas"</span><span class="p">)</span>

<span class="c1"># set the seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># load data</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"data/wdbc_unscaled.csv"</span><span class="p">)</span>
<span class="c1"># re-label Class "M" as "Malignant", and Class "B" as "Benign"</span>
<span class="n">cancer</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span>
    <span class="s2">"M"</span> <span class="p">:</span> <span class="s2">"Malignant"</span><span class="p">,</span>
    <span class="s2">"B"</span> <span class="p">:</span> <span class="s2">"Benign"</span>
<span class="p">})</span>

<span class="c1"># create scatter plot of tumor cell concavity versus smoothness,</span>
<span class="c1"># labeling the points be diagnosis class</span>

<span class="n">perim_concav</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">cancer</span><span class="p">)</span><span class="o">.</span><span class="n">mark_circle</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">"Smoothness"</span><span class="p">)</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">zero</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">"Concavity"</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Color</span><span class="p">(</span><span class="s2">"Class"</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Diagnosis"</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">perim_concav</span>
</pre></div>
</div>
</div>
</div>
<figure class="align-default" id="fig-06-precode">
<div class="output text_html">
<style>
  <ins>#altair-viz-ae84eff69d1e4dbd8f4fd334d19890d6.vega-embed</ins><del>#altair-viz-5064f153038d4364b9295990d4ea79ff.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-ae84eff69d1e4dbd8f4fd334d19890d6.vega-embed</ins><del>#altair-viz-5064f153038d4364b9295990d4ea79ff.vega-embed</del> details,
  <ins>#altair-viz-ae84eff69d1e4dbd8f4fd334d19890d6.vega-embed</ins><del>#altair-viz-5064f153038d4364b9295990d4ea79ff.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-ae84eff69d1e4dbd8f4fd334d19890d6"><div id="altair-viz-5064f153038d4364b9295990d4ea79ff"></div><img src="prerendered/fig-06-precode.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Scatter plot of tumor cell concavity versus smoothness colored by diagnosis label.</span><a class="headerlink" href="#fig-06-precode" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
<section id="create-the-train-test-split">
<h3><span class="section-number">6.5.1. </span>Create the train / test split<a class="headerlink" href="#create-the-train-test-split" title="Permalink to this heading">#</a></h3>
<p>Once we have decided on a predictive question to answer and done some
preliminary exploration, the very next thing to do is to split the data into
the training and test sets. Typically, the training set is between 50% and 95% of
the data, while the test set is the remaining 5% to 50%; the intuition is that
you want to trade off between training an accurate model (by using a larger
training data set) and getting an accurate evaluation of its performance (by
using a larger test data set). Here, we will use 75% of the data for training,
and 25% for testing.</p>
<p id="index-16">The <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> handles the procedure of splitting
the data for us. We can specify two very important parameters when using <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> to ensure
that the accuracy estimates from the test data are reasonable. First,
setting <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> (which is the default) means the data will be shuffled before splitting,
which ensures that any ordering present
in the data does not influence the data that ends up in the training and testing sets.
Second, by specifying the <code class="docutils literal notranslate"><span class="pre">stratify</span></code> parameter to be the response variable in the training set,
it <strong>stratifies</strong> the data by the class label, to ensure that roughly
the same proportion of each class ends up in both the training and testing sets. For example,
in our data set, roughly 63% of the
observations are from the benign class (<code class="docutils literal notranslate"><span class="pre">Benign</span></code>), and 37% are from the malignant class (<code class="docutils literal notranslate"><span class="pre">Malignant</span></code>),
so specifying <code class="docutils literal notranslate"><span class="pre">stratify</span></code> as the class column ensures that roughly 63% of the training data are benign,
37% of the training data are malignant,
and the same proportions exist in the testing data.</p>
<p>Let’s use the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function to create the training and testing sets.
We first need to import the function from the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> package. Then
we will specify that <code class="docutils literal notranslate"><span class="pre">train_size=0.75</span></code> so that 75% of our original data set ends up
in the training set. We will also set the <code class="docutils literal notranslate"><span class="pre">stratify</span></code> argument to the categorical label variable
(here, <code class="docutils literal notranslate"><span class="pre">cancer["Class"]</span></code>) to ensure that the training and testing subsets contain the
right proportions of each category of observation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">cancer_train</span><span class="p">,</span> <span class="n">cancer_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">cancer_train</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 426 entries, 196 to 296
Data columns (total 12 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   ID                 426 non-null    int64  
 1   Class              426 non-null    object 
 2   Radius             426 non-null    float64
 3   Texture            426 non-null    float64
 4   Perimeter          426 non-null    float64
 5   Area               426 non-null    float64
 6   Smoothness         426 non-null    float64
 7   Compactness        426 non-null    float64
 8   Concavity          426 non-null    float64
 9   Concave_Points     426 non-null    float64
 10  Symmetry           426 non-null    float64
 11  Fractal_Dimension  426 non-null    float64
dtypes: float64(10), int64(1), object(1)
memory usage: 43.3+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_test</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 143 entries, 116 to 15
Data columns (total 12 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   ID                 143 non-null    int64  
 1   Class              143 non-null    object 
 2   Radius             143 non-null    float64
 3   Texture            143 non-null    float64
 4   Perimeter          143 non-null    float64
 5   Area               143 non-null    float64
 6   Smoothness         143 non-null    float64
 7   Compactness        143 non-null    float64
 8   Concavity          143 non-null    float64
 9   Concave_Points     143 non-null    float64
 10  Symmetry           143 non-null    float64
 11  Fractal_Dimension  143 non-null    float64
dtypes: float64(10), int64(1), object(1)
memory usage: 14.5+ KB
</pre></div>
</div>
</div>
</div>
<p id="index-17">We can see from the <code class="docutils literal notranslate"><span class="pre">info</span></code> method above that the training set contains <span class="pasted-text">426</span> observations,
while the test set contains <span class="pasted-text">143</span> observations. This corresponds to
a train / test split of 75% / 25%, as desired. Recall from <a class="reference internal link-to-diff" href="classification1.html#classification1"><span class="std std-numref">Chapter 5</span></a>
that we use the <code class="docutils literal notranslate"><span class="pre">info</span></code> method to preview the number of rows, the variable names, their data types, and
missing entries of a data frame.</p>
<p id="index-18">We can use the <code class="docutils literal notranslate"><span class="pre">value_counts</span></code> method with the <code class="docutils literal notranslate"><span class="pre">normalize</span></code> argument set to <code class="docutils literal notranslate"><span class="pre">True</span></code>
to find the percentage of malignant and benign classes
in <code class="docutils literal notranslate"><span class="pre">cancer_train</span></code>. We see about <span class="pasted-text">63</span>% of the training
data are benign and <span class="pasted-text">37</span>%
are malignant, indicating that our class proportions were roughly preserved when we split the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_train</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Class
Benign       0.626761
Malignant    0.373239
Name: proportion, dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="preprocess-the-data">
<h3><span class="section-number">6.5.2. </span>Preprocess the data<a class="headerlink" href="#preprocess-the-data" title="Permalink to this heading">#</a></h3>
<p>As we mentioned in the last chapter, K-nearest neighbors is sensitive to the scale of the predictors,
so we should perform some preprocessing to standardize them. An
additional consideration we need to take when doing this is that we should
create the standardization preprocessor using <strong>only the training data</strong>. This ensures that
our test data does not influence any aspect of our model training. Once we have
created the standardization preprocessor, we can then apply it separately to both the
training and test data sets.</p>
<p id="index-19">Fortunately, <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> helps us handle this properly as long as we wrap our
analysis steps in a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>, as in <a class="reference internal link-to-diff" href="classification1.html#classification1"><span class="std std-numref">Chapter 5</span></a>.
So below we construct and prepare
the preprocessor using <code class="docutils literal notranslate"><span class="pre">make_column_transformer</span></code> just as before.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">make_column_transformer</span>

<span class="n">cancer_preprocessor</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">(</span>
    <span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="p">[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-classifier">
<h3><span class="section-number">6.5.3. </span>Train the classifier<a class="headerlink" href="#train-the-classifier" title="Permalink to this heading">#</a></h3>
<p>Now that we have split our original data set into training and test sets, we
can create our K-nearest neighbors classifier with only the training set using
the technique we learned in the previous chapter. For now, we will just choose
the number <span class="math notranslate nohighlight">\(K\)</span> of neighbors to be 3, and use only the concavity and smoothness predictors by
selecting them from the <code class="docutils literal notranslate"><span class="pre">cancer_train</span></code> data frame.
We will first import the <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> model and <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.
Then as before we will create a model object, combine
the model object and preprocessor into a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> using the <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> function, and then finally
use the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method to build the classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">cancer_train</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cancer_train</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>

<span class="n">knn_pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">cancer_preprocessor</span><span class="p">,</span> <span class="n">knn</span><span class="p">)</span>
<span class="n">knn_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">knn_pipeline</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div class="sk-top-container" id="sk-container-id-1"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
                                                  StandardScaler(),
                                                  ['Smoothness',
                                                   'Concavity'])])),
                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=3))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox"/><label class="sk-toggleable__label sk-toggleable__label-arrow" for="sk-estimator-id-1">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
                                                  StandardScaler(),
                                                  ['Smoothness',
                                                   'Concavity'])])),
                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=3))])</pre></div></div></div><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"/><label class="sk-toggleable__label sk-toggleable__label-arrow" for="sk-estimator-id-2">columntransformer: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[('standardscaler', StandardScaler(),
                                 ['Smoothness', 'Concavity'])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"/><label class="sk-toggleable__label sk-toggleable__label-arrow" for="sk-estimator-id-3">standardscaler</label><div class="sk-toggleable__content"><pre>['Smoothness', 'Concavity']</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox"/><label class="sk-toggleable__label sk-toggleable__label-arrow" for="sk-estimator-id-4">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox"/><label class="sk-toggleable__label sk-toggleable__label-arrow" for="sk-estimator-id-5">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div></div></div></div></div>
</div>
</section>
<section id="predict-the-labels-in-the-test-set">
<h3><span class="section-number">6.5.4. </span>Predict the labels in the test set<a class="headerlink" href="#predict-the-labels-in-the-test-set" title="Permalink to this heading">#</a></h3>
<p id="index-20">Now that we have a K-nearest neighbors classifier object, we can use it to
predict the class labels for our test set and
augment the original test data with a column of predictions.
The <code class="docutils literal notranslate"><span class="pre">Class</span></code> variable contains the actual
diagnoses, while the <code class="docutils literal notranslate"><span class="pre">predicted</span></code> contains the predicted diagnoses from the
classifier. Note that below we print out just the <code class="docutils literal notranslate"><span class="pre">ID</span></code>, <code class="docutils literal notranslate"><span class="pre">Class</span></code>, and <code class="docutils literal notranslate"><span class="pre">predicted</span></code>
variables in the output data frame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">]</span> <span class="o">=</span> <span class="n">knn_pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">cancer_test</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]])</span>
<span class="n">cancer_test</span><span class="p">[[</span><span class="s2">"ID"</span><span class="p">,</span> <span class="s2">"Class"</span><span class="p">,</span> <span class="s2">"predicted"</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>ID</th>
<th>Class</th>
<th>predicted</th>
</tr>
</thead>
<tbody>
<tr>
<th>116</th>
<td>864726</td>
<td>Benign</td>
<td>Malignant</td>
</tr>
<tr>
<th>146</th>
<td>869691</td>
<td>Malignant</td>
<td>Malignant</td>
</tr>
<tr>
<th>86</th>
<td>86135501</td>
<td>Malignant</td>
<td>Malignant</td>
</tr>
<tr>
<th>12</th>
<td>846226</td>
<td>Malignant</td>
<td>Malignant</td>
</tr>
<tr>
<th>105</th>
<td>863030</td>
<td>Malignant</td>
<td>Malignant</td>
</tr>
<tr>
<th>...</th>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<th>244</th>
<td>884180</td>
<td>Malignant</td>
<td>Malignant</td>
</tr>
<tr>
<th>23</th>
<td>851509</td>
<td>Malignant</td>
<td>Malignant</td>
</tr>
<tr>
<th>125</th>
<td>86561</td>
<td>Benign</td>
<td>Benign</td>
</tr>
<tr>
<th>281</th>
<td>8912055</td>
<td>Benign</td>
<td>Benign</td>
</tr>
<tr>
<th>15</th>
<td>84799002</td>
<td>Malignant</td>
<td>Malignant</td>
</tr>
</tbody>
</table>
<p>143 rows × 3 columns</p>
</div></div></div>
</div>
</section>
<section id="evaluate-performance">
<span id="eval-performance-clasfcn2"></span><h3><span class="section-number">6.5.5. </span>Evaluate performance<a class="headerlink" href="#evaluate-performance" title="Permalink to this heading">#</a></h3>
<p id="index-21">Finally, we can assess our classifier’s performance. First, we will examine accuracy.
To do this we will use the <code class="docutils literal notranslate"><span class="pre">score</span></code> method, specifying two arguments:
predictors and the actual labels. We pass the same test data
for the predictors that we originally passed into <code class="docutils literal notranslate"><span class="pre">predict</span></code> when making predictions,
and we provide the actual labels via the <code class="docutils literal notranslate"><span class="pre">cancer_test["Class"]</span></code> series.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span>
    <span class="n">cancer_test</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]],</span>
    <span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8951048951048951
</pre></div>
</div>
</div>
</div>
<p>The output shows that the estimated accuracy of the classifier on the test data
was <span class="pasted-text">90</span>%. To compute the precision and recall, we can use the
<code class="docutils literal notranslate"><span class="pre">precision_score</span></code> and <code class="docutils literal notranslate"><span class="pre">recall_score</span></code> functions from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. We specify
the true labels from the <code class="docutils literal notranslate"><span class="pre">Class</span></code> variable as the <code class="docutils literal notranslate"><span class="pre">y_true</span></code> argument, the predicted
labels from the <code class="docutils literal notranslate"><span class="pre">predicted</span></code> variable as the <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> argument,
and which label should be considered to be positive via the <code class="docutils literal notranslate"><span class="pre">pos_label</span></code> argument.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">precision_score</span>

<span class="n">precision_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">],</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">],</span>
    <span class="n">pos_label</span><span class="o">=</span><span class="s2">"Malignant"</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8275862068965517
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">recall_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">],</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">],</span>
    <span class="n">pos_label</span><span class="o">=</span><span class="s2">"Malignant"</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9056603773584906
</pre></div>
</div>
</div>
</div>
<p>The output shows that the estimated precision and recall of the classifier on the test
data was <span class="pasted-text">83</span>% and <span class="pasted-text">91</span>%, respectively.
Finally, we can look at the <em>confusion matrix</em> for the classifier
using the <code class="docutils literal notranslate"><span class="pre">crosstab</span></code> function from <code class="docutils literal notranslate"><span class="pre">pandas</span></code>. The <code class="docutils literal notranslate"><span class="pre">crosstab</span></code> function takes two
arguments: the actual labels first, then the predicted labels second. Note that
<code class="docutils literal notranslate"><span class="pre">crosstab</span></code> orders its columns alphabetically, but the positive label is still <code class="docutils literal notranslate"><span class="pre">Malignant</span></code>,
even if it is not in the top left corner as in the example confusion matrix earlier in this chapter.</p>
<div class="cell docutils container" id="index-22">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span>
    <span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">],</span>
    <span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>predicted</th>
<th>Benign</th>
<th>Malignant</th>
</tr>
<tr>
<th>Class</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Benign</th>
<td>80</td>
<td>10</td>
</tr>
<tr>
<th>Malignant</th>
<td>5</td>
<td>48</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
<p>The confusion matrix shows <span class="pasted-text">48</span> observations were correctly predicted
as malignant, and <span class="pasted-text">80</span> were correctly predicted as benign.
It also shows that the classifier made some mistakes; in particular,
it classified <span class="pasted-text">5</span> observations as benign when they were actually malignant,
and <span class="pasted-text">10</span> observations as malignant when they were actually benign.
Using our formulas from earlier, we see that the accuracy, precision, and recall agree with what Python reported.</p>
<div class="pasted-math math notranslate nohighlight">
\[\displaystyle \mathrm{accuracy} = \frac{\mathrm{number \; of  \; correct  \; predictions}}{\mathrm{total \;  number \;  of  \; predictions}} = \frac{80+48}{80+48+10+5} = 89.51\]</div>
<div class="pasted-math math notranslate nohighlight">
\[\displaystyle \mathrm{precision} = \frac{\mathrm{number \; of  \; correct  \; positive \; predictions}}{\mathrm{total \;  number \;  of  \; positive \; predictions}} = \frac{48}{48+10} = 82.76\]</div>
<div class="pasted-math math notranslate nohighlight">
\[\displaystyle \mathrm{recall} = \frac{\mathrm{number \; of  \; correct  \; positive \; predictions}}{\mathrm{total \;  number \;  of  \; positive \; test \; set \; observations}} = \frac{48}{48+5} = 90.57\]</div>
</section>
<section id="critically-analyze-performance">
<h3><span class="section-number">6.5.6. </span>Critically analyze performance<a class="headerlink" href="#critically-analyze-performance" title="Permalink to this heading">#</a></h3>
<p>We now know that the classifier was <span class="pasted-text">90</span>% accurate
on the test data set, and had a precision of <span class="pasted-text">83</span>% and
a recall of <span class="pasted-text">91</span>%.
That sounds pretty good! Wait, <em>is</em> it good?
Or do we need something higher?</p>
<p id="index-23">In general, a <em>good</em> value for accuracy (as well as precision and recall, if applicable)
depends on the application; you must critically analyze your accuracy in the context of the problem
you are solving. For example, if we were building a classifier for a kind of tumor that is benign 99%
of the time, a classifier with 99% accuracy is not terribly impressive (just always guess benign!).
And beyond just accuracy, we need to consider the precision and recall: as mentioned
earlier, the <em>kind</em> of mistake the classifier makes is
important in many applications as well. In the previous example with 99% benign observations, it might be very bad for the
classifier to predict “benign” when the actual class is “malignant” (a false negative), as this
might result in a patient not receiving appropriate medical attention. In other
words, in this context, we need the classifier to have a <em>high recall</em>. On the
other hand, it might be less bad for the classifier to guess “malignant” when
the actual class is “benign” (a false positive), as the patient will then likely see a doctor who
can provide an expert diagnosis. In other words, we are fine with sacrificing
some precision in the interest of achieving high recall. This is why it is
important not only to look at accuracy, but also the confusion matrix.</p>
<p id="index-24">However, there is always an easy baseline that you can compare to for any
classification problem: the <em>majority classifier</em>. The majority classifier
<em>always</em> guesses the majority class label from the training data, regardless of
the predictor variables’ values.  It helps to give you a sense of
scale when considering accuracies. If the majority classifier obtains a 90%
accuracy on a problem, then you might hope for your K-nearest neighbors
classifier to do better than that. If your classifier provides a significant
improvement upon the majority classifier, this means that at least your method
is extracting some useful information from your predictor variables.  Be
careful though: improving on the majority classifier does not <em>necessarily</em>
mean the classifier is working well enough for your application.</p>
<p>As an example, in the breast cancer data, recall the proportions of benign and malignant
observations in the training data are as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_train</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Class
Benign       0.626761
Malignant    0.373239
Name: proportion, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Since the benign class represents the majority of the training data,
the majority classifier would <em>always</em> predict that a new observation
is benign. The estimated accuracy of the majority classifier is usually
fairly close to the majority class proportion in the training data.
In this case, we would suspect that the majority classifier will have
an accuracy of around <span class="pasted-text">63</span>%.
The K-nearest neighbors classifier we built does quite a bit better than this,
with an accuracy of <span class="pasted-text">90</span>%.
This means that from the perspective of accuracy,
the K-nearest neighbors classifier improved quite a bit on the basic
majority classifier. Hooray! But we still need to be cautious; in
this application, it is likely very important not to misdiagnose any malignant tumors to avoid missing
patients who actually need medical care. The confusion matrix above shows
that the classifier does, indeed, misdiagnose a significant number of
malignant tumors as benign (<span class="pasted-text">5</span> out of <span class="pasted-text">53</span> malignant tumors, or <span class="pasted-text">9</span>%!).
Therefore, even though the accuracy improved upon the majority classifier,
our critical analysis suggests that this classifier may not have appropriate performance
for the application.</p>
</section>
</section>
<section id="tuning-the-classifier">
<h2><span class="section-number">6.6. </span>Tuning the classifier<a class="headerlink" href="#tuning-the-classifier" title="Permalink to this heading">#</a></h2>
<span class="target" id="index-25"></span><p id="index-26">The vast majority of predictive models in statistics and machine learning have
<em>parameters</em>. A <em>parameter</em>
is a number you have to pick in advance that determines
some aspect of how the model behaves. For example, in the K-nearest neighbors
classification algorithm, <span class="math notranslate nohighlight">\(K\)</span> is a parameter that we have to pick
that determines how many neighbors participate in the class vote.
By picking different values of <span class="math notranslate nohighlight">\(K\)</span>, we create different classifiers
that make different predictions.</p>
<p>So then, how do we pick the <em>best</em> value of <span class="math notranslate nohighlight">\(K\)</span>, i.e., <em>tune</em> the model?
And is it possible to make this selection in a principled way?  In this book,
we will focus on maximizing the accuracy of the classifier. Ideally,
we want somehow to maximize the accuracy of our classifier on data <em>it
hasn’t seen yet</em>. But we cannot use our test data set in the process of building
our model. So we will play the same trick we did before when evaluating
our classifier: we’ll split our <em>training data itself</em> into two subsets,
use one to train the model, and then use the other to evaluate it.
In this section, we will cover the details of this procedure, as well as
how to use it to help you pick a good parameter value for your classifier.</p>
<p><strong>And remember:</strong> don’t touch the test set during the tuning process. Tuning is a part of model training!</p>
<section id="cross-validation">
<h3><span class="section-number">6.6.1. </span>Cross-validation<a class="headerlink" href="#cross-validation" title="Permalink to this heading">#</a></h3>
<p id="index-27">The first step in choosing the parameter <span class="math notranslate nohighlight">\(K\)</span> is to be able to evaluate the
classifier using only the training data. If this is possible, then we can compare
the classifier’s performance for different values of <span class="math notranslate nohighlight">\(K\)</span>—and pick the best—using
only the training data. As suggested at the beginning of this section, we will
accomplish this by splitting the training data, training on one subset, and evaluating
on the other. The subset of training data used for evaluation is often called the <strong>validation set</strong>.</p>
<p>There is, however, one key difference from the train/test split
that we performed earlier. In particular, we were forced to make only a <em>single split</em>
of the data. This is because at the end of the day, we have to produce a single classifier.
If we had multiple different splits of the data into training and testing data,
we would produce multiple different classifiers.
But while we are tuning the classifier, we are free to create multiple classifiers
based on multiple splits of the training data, evaluate them, and then choose a parameter
value based on <strong><em>all</em></strong> of the different results. If we just split our overall training
data <em>once</em>, our best parameter choice will depend strongly on whatever data
was lucky enough to end up in the validation set. Perhaps using multiple
different train/validation splits, we’ll get a better estimate of accuracy,
which will lead to a better choice of the number of neighbors <span class="math notranslate nohighlight">\(K\)</span> for the
overall set of training data.</p>
<p>Let’s investigate this idea in Python! In particular, we will generate five different train/validation
splits of our overall training data, train five different K-nearest neighbors
models, and evaluate their accuracy. We will start with just a single
split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the 25/75 split of the *training data* into sub-training and validation</span>
<span class="n">cancer_subtrain</span><span class="p">,</span> <span class="n">cancer_validation</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">cancer_train</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer_train</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># fit the model on the sub-training data</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cancer_subtrain</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cancer_subtrain</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="n">knn_pipeline</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">cancer_preprocessor</span><span class="p">,</span> <span class="n">knn</span><span class="p">)</span>
<span class="n">knn_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># compute the score on validation data</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">knn_pipeline</span><span class="o">.</span><span class="n">score</span><span class="p">(</span>
    <span class="n">cancer_validation</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]],</span>
    <span class="n">cancer_validation</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">acc</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.897196261682243
</pre></div>
</div>
</div>
</div>
<p>The accuracy estimate using this split is <span class="pasted-text">89.7</span>%.
Now we repeat the above code 4 more times, which generates 4 more splits.
Therefore we get five different shuffles of the data, and therefore five different values for
accuracy: <span class="pasted-text">[89.7%, 88.8%, 87.9%, 86.0%, 87.9%]</span>. None of these values are
necessarily “more correct” than any other; they’re
just five estimates of the true, underlying accuracy of our classifier built
using our overall training data. We can combine the estimates by taking their
average (here <span class="pasted-text">88.0</span>%) to try to get a single assessment of our
classifier’s accuracy; this has the effect of reducing the influence of any one
(un)lucky validation set on the estimate.</p>
<p id="index-28">In practice, we don’t use random splits, but rather use a more structured
splitting procedure so that each observation in the data set is used in a
validation set only a single time. The name for this strategy is
<strong>cross-validation</strong>.  In <strong>cross-validation</strong>, we split our <strong>overall training
data</strong> into <span class="math notranslate nohighlight">\(C\)</span> evenly sized chunks. Then, iteratively use <span class="math notranslate nohighlight">\(1\)</span> chunk as the
<strong>validation set</strong> and combine the remaining <span class="math notranslate nohighlight">\(C-1\)</span> chunks
as the <strong>training set</strong>.
This procedure is shown in <a class="reference internal" href="#fig-06-cv-image"><span class="std std-numref">Fig. 6.4</span></a>.
Here, <span class="math notranslate nohighlight">\(C=5\)</span> different chunks of the data set are used,
resulting in 5 different choices for the <strong>validation set</strong>; we call this
<em>5-fold</em> cross-validation.</p>
<figure class="align-default" id="fig-06-cv-image">
<img alt="_images/cv.png" src="_images/cv.png"/>
<figcaption>
<p><span class="caption-number">Fig. 6.4 </span><span class="caption-text">5-fold cross-validation.</span><a class="headerlink" href="#fig-06-cv-image" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p id="index-29">To perform 5-fold cross-validation in Python with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, we use another
function: <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code>. This function requires that we specify
a modelling <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> as the <code class="docutils literal notranslate"><span class="pre">estimator</span></code> argument,
the number of folds as the <code class="docutils literal notranslate"><span class="pre">cv</span></code> argument,
and the training data predictors and labels as the <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> arguments.
Since the <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> function outputs a dictionary, we use <code class="docutils literal notranslate"><span class="pre">pd.DataFrame</span></code> to convert it to a <code class="docutils literal notranslate"><span class="pre">pandas</span></code>
dataframe for better visualization.
Note that the <code class="docutils literal notranslate"><span class="pre">cross_validate</span></code> function handles stratifying the classes in
each train and validate fold automatically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">cancer_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">cancer_preprocessor</span><span class="p">,</span> <span class="n">knn</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cancer_train</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cancer_train</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="n">cv_5_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span>
        <span class="n">estimator</span><span class="o">=</span><span class="n">cancer_pipe</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">cv_5_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>fit_time</th>
<th>score_time</th>
<th>test_score</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td><ins class="diff">0.004522</ins><del class="diff">0.004346</del></td>
<td><ins class="diff">0.005873</ins><del class="diff">0.006559</del></td>
<td>0.837209</td>
</tr>
<tr>
<th>1</th>
<td><ins class="diff">0.003844</ins><del class="diff">0.003702</del></td>
<td><ins class="diff">0.005518</ins><del class="diff">0.005461</del></td>
<td>0.870588</td>
</tr>
<tr>
<th>2</th>
<td><ins class="diff">0.003665</ins><del class="diff">0.003611</del></td>
<td><ins class="diff">0.005449</ins><del class="diff">0.005531</del></td>
<td>0.894118</td>
</tr>
<tr>
<th>3</th>
<td><ins class="diff">0.003804</ins><del class="diff">0.003764</del></td>
<td><ins class="diff">0.005477</ins><del class="diff">0.005418</del></td>
<td>0.870588</td>
</tr>
<tr>
<th>4</th>
<td><ins class="diff">0.003678</ins><del class="diff">0.003544</del></td>
<td><ins class="diff">0.005441</ins><del class="diff">0.005412</del></td>
<td>0.882353</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
<span class="target" id="index-30"></span><p id="index-31">The validation scores we are interested in are contained in the <code class="docutils literal notranslate"><span class="pre">test_score</span></code> column.
We can then aggregate the <em>mean</em> and <em>standard error</em>
of the classifier’s validation accuracy across the folds.
You should consider the mean (<code class="docutils literal notranslate"><span class="pre">mean</span></code>) to be the estimated accuracy, while the standard
error (<code class="docutils literal notranslate"><span class="pre">sem</span></code>) is a measure of how uncertain we are in that mean value. A detailed treatment of this
is beyond the scope of this chapter; but roughly, if your estimated mean is <span class="pasted-text">0.87</span> and standard
error is <span class="pasted-text">0.01</span>, you can expect the <em>true</em> average accuracy of the
classifier to be somewhere roughly between <span class="pasted-text">86</span>% and <span class="pasted-text">88</span>% (although it may
fall outside this range). You may ignore the other columns in the metrics data frame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv_5_metrics</span> <span class="o">=</span> <span class="n">cv_5_df</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s2">"mean"</span><span class="p">,</span> <span class="s2">"sem"</span><span class="p">])</span>
<span class="n">cv_5_metrics</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>fit_time</th>
<th>score_time</th>
<th>test_score</th>
</tr>
</thead>
<tbody>
<tr>
<th>mean</th>
<td><ins class="diff">0.003903</ins><del class="diff">0.003793</del></td>
<td><ins class="diff">0.005552</ins><del class="diff">0.005676</del></td>
<td>0.870971</td>
</tr>
<tr>
<th>sem</th>
<td><ins class="diff">0.000159</ins><del class="diff">0.000143</del></td>
<td><ins class="diff">0.000081</ins><del class="diff">0.000222</del></td>
<td>0.009501</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
<p>We can choose any number of folds, and typically the more we use the better our
accuracy estimate will be (lower standard error). However, we are limited
by computational power: the
more folds we choose, the  more computation it takes, and hence the more time
it takes to run the analysis. So when you do cross-validation, you need to
consider the size of the data, the speed of the algorithm (e.g., K-nearest
neighbors), and the speed of your computer. In practice, this is a
trial-and-error process, but typically <span class="math notranslate nohighlight">\(C\)</span> is chosen to be either 5 or 10. Here
we will try 10-fold cross-validation to see if we get a lower standard error.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv_10</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span>
        <span class="n">estimator</span><span class="o">=</span><span class="n">cancer_pipe</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">cv_10_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv_10</span><span class="p">)</span>
<span class="n">cv_10_metrics</span> <span class="o">=</span> <span class="n">cv_10_df</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s2">"mean"</span><span class="p">,</span> <span class="s2">"sem"</span><span class="p">])</span>
<span class="n">cv_10_metrics</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>fit_time</th>
<th>score_time</th>
<th>test_score</th>
</tr>
</thead>
<tbody>
<tr>
<th>mean</th>
<td><ins class="diff">0.003690</ins><del class="diff">0.003605</del></td>
<td><ins class="diff">0.004243</ins><del class="diff">0.004197</del></td>
<td>0.884939</td>
</tr>
<tr>
<th>sem</th>
<td><ins class="diff">0.000028</ins><del class="diff">0.000034</del></td>
<td><ins class="diff">0.000029</ins><del class="diff">0.000038</del></td>
<td>0.006718</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
<p id="index-32">In this case, using 10-fold instead of 5-fold cross validation did
reduce the standard error very slightly. In fact, due to the randomness in how the data are split, sometimes
you might even end up with a <em>higher</em> standard error when increasing the number of folds!
We can make the reduction in standard error more dramatic by increasing the number of folds
by a large amount. In the following code we show the result when <span class="math notranslate nohighlight">\(C = 50\)</span>;
picking such a large number of folds can take a long time to run in practice,
so we usually stick to 5 or 10.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv_50_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span>
        <span class="n">estimator</span><span class="o">=</span><span class="n">cancer_pipe</span><span class="p">,</span>
        <span class="n">cv</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">cv_50_metrics</span> <span class="o">=</span> <span class="n">cv_50_df</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s2">"mean"</span><span class="p">,</span> <span class="s2">"sem"</span><span class="p">])</span>
<span class="n">cv_50_metrics</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>fit_time</th>
<th>score_time</th>
<th>test_score</th>
</tr>
</thead>
<tbody>
<tr>
<th>mean</th>
<td><ins class="diff">0.003614</ins><del class="diff">0.003556</del></td>
<td><ins class="diff">0.003137</ins><del class="diff">0.003110</del></td>
<td>0.888056</td>
</tr>
<tr>
<th>sem</th>
<td><ins class="diff">0.000015</ins><del class="diff">0.000019</del></td>
<td><ins class="diff">0.000009</ins><del class="diff">0.000015</del></td>
<td>0.003005</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
</section>
<section id="parameter-value-selection">
<h3><span class="section-number">6.6.2. </span>Parameter value selection<a class="headerlink" href="#parameter-value-selection" title="Permalink to this heading">#</a></h3>
<p>Using 5- and 10-fold cross-validation, we have estimated that the prediction
accuracy of our classifier is somewhere around <span class="pasted-text">88</span>%.
Whether that is good or not
depends entirely on the downstream application of the data analysis. In the
present situation, we are trying to predict a tumor diagnosis, with expensive,
damaging chemo/radiation therapy or patient death as potential consequences of
misprediction. Hence, we might like to
do better than <span class="pasted-text">88</span>% for this application.</p>
<p>In order to improve our classifier, we have one choice of parameter: the number of
neighbors, <span class="math notranslate nohighlight">\(K\)</span>. Since cross-validation helps us evaluate the accuracy of our
classifier, we can use cross-validation to calculate an accuracy for each value
of <span class="math notranslate nohighlight">\(K\)</span> in a reasonable range, and then pick the value of <span class="math notranslate nohighlight">\(K\)</span> that gives us the
best accuracy. The <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> package collection provides built-in
functionality, named <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>, to automatically handle the details for us.
Before we use <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>, we need to create a new pipeline
with a <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> that has the number of neighbors left unspecified.</p>
<span class="target" id="index-33"></span><div class="cell docutils container" id="index-34">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">cancer_tune_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">cancer_preprocessor</span><span class="p">,</span> <span class="n">knn</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next we specify the grid of parameter values that we want to try for
each tunable parameter. We do this in a Python dictionary: the key is
the identifier of the parameter to tune, and the value is a list of parameter values
to try when tuning. We can find the “identifier” of a parameter by using
the <code class="docutils literal notranslate"><span class="pre">get_params</span></code> method on the pipeline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_tune_pipe</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{'memory': None,
 'steps': [('columntransformer',
   ColumnTransformer(transformers=[('standardscaler', StandardScaler(),
                                    ['Smoothness', 'Concavity'])])),
  ('kneighborsclassifier', KNeighborsClassifier())],
 'verbose': False,
 'columntransformer': ColumnTransformer(transformers=[('standardscaler', StandardScaler(),
                                  ['Smoothness', 'Concavity'])]),
 'kneighborsclassifier': KNeighborsClassifier(),
 'columntransformer__n_jobs': None,
 'columntransformer__remainder': 'drop',
 'columntransformer__sparse_threshold': 0.3,
 'columntransformer__transformer_weights': None,
 'columntransformer__transformers': [('standardscaler',
   StandardScaler(),
   ['Smoothness', 'Concavity'])],
 'columntransformer__verbose': False,
 'columntransformer__verbose_feature_names_out': True,
 'columntransformer__standardscaler': StandardScaler(),
 'columntransformer__standardscaler__copy': True,
 'columntransformer__standardscaler__with_mean': True,
 'columntransformer__standardscaler__with_std': True,
 'kneighborsclassifier__algorithm': 'auto',
 'kneighborsclassifier__leaf_size': 30,
 'kneighborsclassifier__metric': 'minkowski',
 'kneighborsclassifier__metric_params': None,
 'kneighborsclassifier__n_jobs': None,
 'kneighborsclassifier__n_neighbors': 5,
 'kneighborsclassifier__p': 2,
 'kneighborsclassifier__weights': 'uniform'}
</pre></div>
</div>
</div>
</div>
<p>Wow, there’s quite a bit of <em>stuff</em> there! If you sift through the muck
a little bit, you will see one parameter identifier that stands out:
<code class="docutils literal notranslate"><span class="pre">"kneighborsclassifier__n_neighbors"</span></code>. This identifier combines the name
of the K nearest neighbors classification step in our pipeline, <code class="docutils literal notranslate"><span class="pre">kneighborsclassifier</span></code>,
with the name of the parameter, <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>.
We now construct the <code class="docutils literal notranslate"><span class="pre">parameter_grid</span></code> dictionary that will tell <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>
what parameter values to try.
Note that you can specify multiple tunable parameters
by creating a dictionary with multiple key-value pairs, but
here we just have to tune the number of neighbors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameter_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"kneighborsclassifier__n_neighbors"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">range</span></code> function in Python that we used above allows us to specify a sequence of values.
The first argument is the starting number (here, <code class="docutils literal notranslate"><span class="pre">1</span></code>),
the second argument is <em>one greater than</em> the final number (here, <code class="docutils literal notranslate"><span class="pre">100</span></code>),
and the third argument is the number to values to skip between steps in the sequence (here, <code class="docutils literal notranslate"><span class="pre">5</span></code>).
So in this case we generate the sequence 1, 6, 11, 16, …, 96.
If we instead specified <code class="docutils literal notranslate"><span class="pre">range(0,</span> <span class="pre">100,</span> <span class="pre">5)</span></code>, we would get the sequence 0, 5, 10, 15, …, 90, 95.
The number 100 is not included because the third argument is <em>one greater than</em> the final possible
number in the sequence. There are two additional useful ways to employ <code class="docutils literal notranslate"><span class="pre">range</span></code>.
If we call <code class="docutils literal notranslate"><span class="pre">range</span></code> with just one argument, Python counts
up to that number starting at 0. So <code class="docutils literal notranslate"><span class="pre">range(4)</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">range(0,</span> <span class="pre">4,</span> <span class="pre">1)</span></code> and generates the sequence 0, 1, 2, 3.
If we call <code class="docutils literal notranslate"><span class="pre">range</span></code> with two arguments, Python counts starting at the first number up to the second number.
So <code class="docutils literal notranslate"><span class="pre">range(1,</span> <span class="pre">4)</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">range(1,</span> <span class="pre">4,</span> <span class="pre">1)</span></code> and generates the sequence <code class="docutils literal notranslate"><span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3</span></code>.</p>
<p id="index-35">Okay! We are finally ready to create the <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> object.
First we import it from the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> package.
Then we pass it the <code class="docutils literal notranslate"><span class="pre">cancer_tune_pipe</span></code> pipeline in the <code class="docutils literal notranslate"><span class="pre">estimator</span></code> argument,
the <code class="docutils literal notranslate"><span class="pre">parameter_grid</span></code> in the <code class="docutils literal notranslate"><span class="pre">param_grid</span></code> argument,
and specify <code class="docutils literal notranslate"><span class="pre">cv=10</span></code> folds. Note that this does not actually run
the tuning yet; just as before, we will have to use the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">cancer_tune_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">cancer_tune_pipe</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">parameter_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we use the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method on the <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> object to begin the tuning process.
We pass the training data predictors and labels as the two arguments to <code class="docutils literal notranslate"><span class="pre">fit</span></code> as usual.
The <code class="docutils literal notranslate"><span class="pre">cv_results_</span></code> attribute of the output contains the resulting cross-validation
accuracy estimate for each choice of <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>, but it isn’t in an easily used
format. We will wrap it in a <code class="docutils literal notranslate"><span class="pre">pd.DataFrame</span></code> to make it easier to understand,
and print the <code class="docutils literal notranslate"><span class="pre">info</span></code> of the result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_tune_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">cancer_train</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]],</span>
    <span class="n">cancer_train</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">accuracies_grid</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cancer_tune_grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">accuracies_grid</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 20 entries, 0 to 19
Data columns (total 19 columns):
 #   Column                                   Non-Null Count  Dtype  
---  ------                                   --------------  -----  
 0   mean_fit_time                            20 non-null     float64
 1   std_fit_time                             20 non-null     float64
 2   mean_score_time                          20 non-null     float64
 3   std_score_time                           20 non-null     float64
 4   param_kneighborsclassifier__n_neighbors  20 non-null     object 
 5   params                                   20 non-null     object 
 6   split0_test_score                        20 non-null     float64
 7   split1_test_score                        20 non-null     float64
 8   split2_test_score                        20 non-null     float64
 9   split3_test_score                        20 non-null     float64
 10  split4_test_score                        20 non-null     float64
 11  split5_test_score                        20 non-null     float64
 12  split6_test_score                        20 non-null     float64
 13  split7_test_score                        20 non-null     float64
 14  split8_test_score                        20 non-null     float64
 15  split9_test_score                        20 non-null     float64
 16  mean_test_score                          20 non-null     float64
 17  std_test_score                           20 non-null     float64
 18  rank_test_score                          20 non-null     int32  
dtypes: float64(16), int32(1), object(2)
memory usage: 3.0+ KB
</pre></div>
</div>
</div>
</div>
<p>There is a lot of information to look at here, but we are most interested
in three quantities: the number of neighbors (<code class="docutils literal notranslate"><span class="pre">param_kneighbors_classifier__n_neighbors</span></code>),
the cross-validation accuracy estimate (<code class="docutils literal notranslate"><span class="pre">mean_test_score</span></code>),
and the standard error of the accuracy estimate. Unfortunately <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> does
not directly output the standard error for each cross-validation accuracy; but
it <em>does</em> output the standard <em>deviation</em> (<code class="docutils literal notranslate"><span class="pre">std_test_score</span></code>). We can compute
the standard error from the standard deviation by dividing it by the square
root of the number of folds, i.e.,</p>
<div class="math notranslate nohighlight">
\[\text{Standard Error} = \frac{\text{Standard Deviation}}{\sqrt{\text{Number of Folds}}}.\]</div>
<p>We will also rename the parameter name column to be a bit more readable,
and drop the now unused <code class="docutils literal notranslate"><span class="pre">std_test_score</span></code> column.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracies_grid</span><span class="p">[</span><span class="s2">"sem_test_score"</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracies_grid</span><span class="p">[</span><span class="s2">"std_test_score"</span><span class="p">]</span> <span class="o">/</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
<span class="n">accuracies_grid</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">accuracies_grid</span><span class="p">[[</span>
        <span class="s2">"param_kneighborsclassifier__n_neighbors"</span><span class="p">,</span>
        <span class="s2">"mean_test_score"</span><span class="p">,</span>
        <span class="s2">"sem_test_score"</span>
    <span class="p">]]</span>
    <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">"param_kneighborsclassifier__n_neighbors"</span><span class="p">:</span> <span class="s2">"n_neighbors"</span><span class="p">})</span>
<span class="p">)</span>
<span class="n">accuracies_grid</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>n_neighbors</th>
<th>mean_test_score</th>
<th>sem_test_score</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>1</td>
<td>0.845127</td>
<td>0.019966</td>
</tr>
<tr>
<th>1</th>
<td>6</td>
<td>0.873200</td>
<td>0.015680</td>
</tr>
<tr>
<th>2</th>
<td>11</td>
<td>0.861517</td>
<td>0.019547</td>
</tr>
<tr>
<th>3</th>
<td>16</td>
<td>0.861573</td>
<td>0.017787</td>
</tr>
<tr>
<th>4</th>
<td>21</td>
<td>0.866279</td>
<td>0.017889</td>
</tr>
<tr>
<th>5</th>
<td>26</td>
<td>0.875637</td>
<td>0.016026</td>
</tr>
<tr>
<th>6</th>
<td>31</td>
<td>0.885050</td>
<td>0.015406</td>
</tr>
<tr>
<th>7</th>
<td>36</td>
<td>0.887375</td>
<td>0.013694</td>
</tr>
<tr>
<th>8</th>
<td>41</td>
<td>0.887375</td>
<td>0.013694</td>
</tr>
<tr>
<th>9</th>
<td>46</td>
<td>0.887320</td>
<td>0.013314</td>
</tr>
<tr>
<th>10</th>
<td>51</td>
<td>0.882669</td>
<td>0.014523</td>
</tr>
<tr>
<th>11</th>
<td>56</td>
<td>0.878018</td>
<td>0.014414</td>
</tr>
<tr>
<th>12</th>
<td>61</td>
<td>0.880343</td>
<td>0.014299</td>
</tr>
<tr>
<th>13</th>
<td>66</td>
<td>0.873200</td>
<td>0.015416</td>
</tr>
<tr>
<th>14</th>
<td>71</td>
<td>0.877962</td>
<td>0.013660</td>
</tr>
<tr>
<th>15</th>
<td>76</td>
<td>0.873200</td>
<td>0.014698</td>
</tr>
<tr>
<th>16</th>
<td>81</td>
<td>0.873200</td>
<td>0.014698</td>
</tr>
<tr>
<th>17</th>
<td>86</td>
<td>0.880288</td>
<td>0.011277</td>
</tr>
<tr>
<th>18</th>
<td>91</td>
<td>0.875581</td>
<td>0.012967</td>
</tr>
<tr>
<th>19</th>
<td>96</td>
<td>0.875581</td>
<td>0.008193</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
<p>We can decide which number of neighbors is best by plotting the accuracy versus <span class="math notranslate nohighlight">\(K\)</span>,
as shown in <a class="reference internal" href="#fig-06-find-k"><span class="std std-numref">Fig. 6.5</span></a>.
Here we are using the shortcut <code class="docutils literal notranslate"><span class="pre">point=True</span></code> to layer a point and line chart.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy_vs_k</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">accuracies_grid</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">point</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">"n_neighbors"</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Neighbors"</span><span class="p">),</span>
    <span class="n">y</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s2">"mean_test_score"</span><span class="p">)</span>
        <span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Accuracy estimate"</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">accuracy_vs_k</span>
</pre></div>
</div>
</div>
</div>
<figure class="align-default" id="fig-06-find-k">
<div class="output text_html">
<style>
  <ins>#altair-viz-10c4004115824ee5b6c59122f8218402.vega-embed</ins><del>#altair-viz-f29222d587af4ac8af7a554cf577677f.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-10c4004115824ee5b6c59122f8218402.vega-embed</ins><del>#altair-viz-f29222d587af4ac8af7a554cf577677f.vega-embed</del> details,
  <ins>#altair-viz-10c4004115824ee5b6c59122f8218402.vega-embed</ins><del>#altair-viz-f29222d587af4ac8af7a554cf577677f.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-10c4004115824ee5b6c59122f8218402"><div id="altair-viz-f29222d587af4ac8af7a554cf577677f"></div><img src="prerendered/fig-06-find-k.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.5 </span><span class="caption-text">Plot of estimated accuracy versus the number of neighbors.</span><a class="headerlink" href="#fig-06-find-k" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
<p>We can also obtain the number of neighbours with the highest accuracy programmatically by accessing
the <code class="docutils literal notranslate"><span class="pre">best_params_</span></code> attribute of the fit <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> object. Note that it is still useful to visualize
the results as we did above since this provides additional information on how the model performance varies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_tune_grid</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{'kneighborsclassifier__n_neighbors': 36}
</pre></div>
</div>
</div>
</div>
<p>Setting the number of
neighbors to <span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span>
provides the highest cross-validation accuracy estimate (<span class="pasted-text">88.7</span>%). But there is no exact or perfect answer here;
any selection from <span class="math notranslate nohighlight">\(K = 30\)</span> to <span class="math notranslate nohighlight">\(80\)</span> or so would be reasonably justified, as all
of these differ in classifier accuracy by a small amount. Remember: the
values you see on this plot are <em>estimates</em> of the true accuracy of our
classifier. Although the
<span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span> value is
higher than the others on this plot,
that doesn’t mean the classifier is actually more accurate with this parameter
value! Generally, when selecting <span class="math notranslate nohighlight">\(K\)</span> (and other parameters for other predictive
models), we are looking for a value where:</p>
<ul class="simple">
<li><p>we get roughly optimal accuracy, so that our model will likely be accurate;</p></li>
<li><p>changing the value to a nearby one (e.g., adding or subtracting a small number) doesn’t decrease accuracy too much, so that our choice is reliable in the presence of uncertainty;</p></li>
<li><p>the cost of training the model is not prohibitive (e.g., in our situation, if <span class="math notranslate nohighlight">\(K\)</span> is too large, predicting becomes expensive!).</p></li>
</ul>
<p>We know that <span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span>
provides the highest estimated accuracy. Further, <a class="reference internal" href="#fig-06-find-k"><span class="std std-numref">Fig. 6.5</span></a> shows that the estimated accuracy
changes by only a small amount if we increase or decrease <span class="math notranslate nohighlight">\(K\)</span> near <span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span>.
And finally, <span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span> does not create a prohibitively expensive
computational cost of training. Considering these three points, we would indeed select
<span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span> for the classifier.</p>
</section>
<section id="under-overfitting">
<h3><span class="section-number">6.6.3. </span>Under/Overfitting<a class="headerlink" href="#under-overfitting" title="Permalink to this heading">#</a></h3>
<p>To build a bit more intuition, what happens if we keep increasing the number of
neighbors <span class="math notranslate nohighlight">\(K\)</span>? In fact, the cross-validation accuracy estimate actually starts to decrease!
Let’s specify a much larger range of values of <span class="math notranslate nohighlight">\(K\)</span> to try in the <code class="docutils literal notranslate"><span class="pre">param_grid</span></code>
argument of <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>. <a class="reference internal" href="#fig-06-lots-of-ks"><span class="std std-numref">Fig. 6.6</span></a> shows a plot of estimated accuracy as
we vary <span class="math notranslate nohighlight">\(K\)</span> from 1 to almost the number of observations in the data set.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">large_param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"kneighborsclassifier__n_neighbors"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">385</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">large_cancer_tune_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">cancer_tune_pipe</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">large_param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>

<span class="n">large_cancer_tune_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">cancer_train</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]],</span>
    <span class="n">cancer_train</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">large_accuracies_grid</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">large_cancer_tune_grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="n">large_accuracy_vs_k</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">large_accuracies_grid</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">point</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s2">"param_kneighborsclassifier__n_neighbors"</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Neighbors"</span><span class="p">),</span>
    <span class="n">y</span><span class="o">=</span><span class="n">alt</span><span class="o">.</span><span class="n">Y</span><span class="p">(</span><span class="s2">"mean_test_score"</span><span class="p">)</span>
        <span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Accuracy estimate"</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">large_accuracy_vs_k</span>
</pre></div>
</div>
</div>
</div>
<figure class="align-default" id="fig-06-lots-of-ks">
<div class="output text_html">
<style>
  <ins>#altair-viz-b0ae56c6849e4ab582537bec1d7ad84a.vega-embed</ins><del>#altair-viz-89985d2b0e0d430cb0ed6311faba78b0.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-b0ae56c6849e4ab582537bec1d7ad84a.vega-embed</ins><del>#altair-viz-89985d2b0e0d430cb0ed6311faba78b0.vega-embed</del> details,
  <ins>#altair-viz-b0ae56c6849e4ab582537bec1d7ad84a.vega-embed</ins><del>#altair-viz-89985d2b0e0d430cb0ed6311faba78b0.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-b0ae56c6849e4ab582537bec1d7ad84a"><div id="altair-viz-89985d2b0e0d430cb0ed6311faba78b0"></div><img src="prerendered/fig-06-lots-of-ks.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.6 </span><span class="caption-text">Plot of accuracy estimate versus number of neighbors for many K values.</span><a class="headerlink" href="#fig-06-lots-of-ks" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
<p id="index-36"><strong>Underfitting:</strong> What is actually happening to our classifier that causes
this? As we increase the number of neighbors, more and more of the training
observations (and those that are farther and farther away from the point) get a
“say” in what the class of a new observation is. This causes a sort of
“averaging effect” to take place, making the boundary between where our
classifier would predict a tumor to be malignant versus benign to smooth out
and become <em>simpler.</em> If you take this to the extreme, setting <span class="math notranslate nohighlight">\(K\)</span> to the total
training data set size, then the classifier will always predict the same label
regardless of what the new observation looks like. In general, if the model
<em>isn’t influenced enough</em> by the training data, it is said to <strong>underfit</strong> the
data.</p>
<p id="index-37"><strong>Overfitting:</strong> In contrast, when we decrease the number of neighbors, each
individual data point has a stronger and stronger vote regarding nearby points.
Since the data themselves are noisy, this causes a more “jagged” boundary
corresponding to a <em>less simple</em> model.  If you take this case to the extreme,
setting <span class="math notranslate nohighlight">\(K = 1\)</span>, then the classifier is essentially just matching each new
observation to its closest neighbor in the training data set. This is just as
problematic as the large <span class="math notranslate nohighlight">\(K\)</span> case, because the classifier becomes unreliable on
new data: if we had a different training set, the predictions would be
completely different.  In general, if the model <em>is influenced too much</em> by the
training data, it is said to <strong>overfit</strong> the data.</p>
<figure class="align-default" id="fig-06-decision-grid-k">
<div class="output text_html">
<style>
  <ins>#altair-viz-5133637b32e645f0afa22f167f409927.vega-embed</ins><del>#altair-viz-db58b696e6614f218076a8dc4a355909.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-5133637b32e645f0afa22f167f409927.vega-embed</ins><del>#altair-viz-db58b696e6614f218076a8dc4a355909.vega-embed</del> details,
  <ins>#altair-viz-5133637b32e645f0afa22f167f409927.vega-embed</ins><del>#altair-viz-db58b696e6614f218076a8dc4a355909.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-5133637b32e645f0afa22f167f409927"><div id="altair-viz-db58b696e6614f218076a8dc4a355909"></div><img src="prerendered/fig-06-decision-grid-k.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.7 </span><span class="caption-text">Effect of K in overfitting and underfitting.</span><a class="headerlink" href="#fig-06-decision-grid-k" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
<p>Both overfitting and underfitting are problematic and will lead to a model that
does not generalize well to new data. When fitting a model, we need to strike a
balance between the two. You can see these two effects in
<a class="reference internal" href="#fig-06-decision-grid-k"><span class="std std-numref">Fig. 6.7</span></a>, which shows how the classifier changes as we
set the number of neighbors <span class="math notranslate nohighlight">\(K\)</span> to 1, 7, 20, and 300.</p>
</section>
<section id="evaluating-on-the-test-set">
<h3><span class="section-number">6.6.4. </span>Evaluating on the test set<a class="headerlink" href="#evaluating-on-the-test-set" title="Permalink to this heading">#</a></h3>
<p>Now that we have tuned the K-NN classifier and set <span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span>,
we are done building the model and it is time to evaluate the quality of its predictions on the held out
test data, as we did earlier in <a class="reference internal" href="#eval-performance-clasfcn2"><span class="std std-numref">Section 6.5.5</span></a>.
We first need to retrain the K-NN classifier
on the entire training data set using the selected number of neighbors.
Fortunately we do not have to do this ourselves manually; <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> does it for
us automatically. To make predictions and assess the estimated accuracy of the best model on the test data, we can use the
<code class="docutils literal notranslate"><span class="pre">score</span></code> and <code class="docutils literal notranslate"><span class="pre">predict</span></code> methods of the fit <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> object. We can then pass those predictions to
the <code class="docutils literal notranslate"><span class="pre">precision</span></code>, <code class="docutils literal notranslate"><span class="pre">recall</span></code>, and <code class="docutils literal notranslate"><span class="pre">crosstab</span></code> functions to assess the estimated precision and recall, and print a confusion matrix.</p>
<div class="cell docutils container" id="index-38">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">]</span> <span class="o">=</span> <span class="n">cancer_tune_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
    <span class="n">cancer_test</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">cancer_tune_grid</span><span class="o">.</span><span class="n">score</span><span class="p">(</span>
    <span class="n">cancer_test</span><span class="p">[[</span><span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">]],</span>
    <span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9090909090909091
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">precision_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">],</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">],</span>
    <span class="n">pos_label</span><span class="o">=</span><span class="s1">'Malignant'</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8846153846153846
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">recall_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">],</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">],</span>
    <span class="n">pos_label</span><span class="o">=</span><span class="s1">'Malignant'</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8679245283018868
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span>
    <span class="n">cancer_test</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">],</span>
    <span class="n">cancer_test</span><span class="p">[</span><span class="s2">"predicted"</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>predicted</th>
<th>Benign</th>
<th>Malignant</th>
</tr>
<tr>
<th>Class</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Benign</th>
<td>84</td>
<td>6</td>
</tr>
<tr>
<th>Malignant</th>
<td>7</td>
<td>46</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
<p>At first glance, this is a bit surprising: the accuracy of the classifier
has not changed much despite tuning the number of neighbors! Our first model
with <span class="math notranslate nohighlight">\(K =\)</span> 3 (before we knew how to tune) had an estimated accuracy of <span class="pasted-text">90</span>%,
while the tuned model with <span class="math notranslate nohighlight">\(K =\)</span> <span class="pasted-text">36</span> had an estimated accuracy
of <span class="pasted-text">91</span>%. Upon examining <a class="reference internal" href="#fig-06-find-k"><span class="std std-numref">Fig. 6.5</span></a> again to see the
cross validation accuracy estimates for a range of neighbors, this result
becomes much less surprising. From <span class="pasted-text">1</span> to around <span class="pasted-text">96</span> neighbors, the cross
validation accuracy estimate varies only by around <span class="pasted-text">3</span>%, with
each estimate having a standard error around <span class="pasted-text">1</span>%.
Since the cross-validation accuracy estimates the test set accuracy,
the fact that the test set accuracy also doesn’t change much is expected.
Also note that the <span class="math notranslate nohighlight">\(K =\)</span> 3 model had a precision
precision of <span class="pasted-text">83</span>% and recall of <span class="pasted-text">91</span>%,
while the tuned model had
a precision of <span class="pasted-text">88</span>% and recall of <span class="pasted-text">87</span>%.
Given that the recall decreased—remember, in this application, recall
is critical to making sure we find all the patients with malignant tumors—the tuned model may actually be <em>less</em> preferred
in this setting. In any case, it is important to think critically about the result of tuning. Models tuned to
maximize accuracy are not necessarily better for a given application.</p>
</section>
</section>
<section id="summary">
<h2><span class="section-number">6.7. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>Classification algorithms use one or more quantitative variables to predict the
value of another categorical variable. In particular, the K-nearest neighbors
algorithm does this by first finding the <span class="math notranslate nohighlight">\(K\)</span> points in the training data
nearest to the new observation, and then returning the majority class vote from
those training observations. We can tune and evaluate a classifier by splitting
the data randomly into a training and test data set. The training set is used
to build the classifier, and we can tune the classifier (e.g., select the number
of neighbors in K-nearest neighbors) by maximizing estimated accuracy via
cross-validation. After we have tuned the model, we can use the test set to
estimate its accuracy.  The overall process is summarized in
<a class="reference internal" href="#fig-06-overview"><span class="std std-numref">Fig. 6.8</span></a>.</p>
<figure class="align-default" id="fig-06-overview">
<img alt="_images/train-test-overview.png" src="_images/train-test-overview.png"/>
<figcaption>
<p><span class="caption-number">Fig. 6.8 </span><span class="caption-text">Overview of K-NN classification.</span><a class="headerlink" href="#fig-06-overview" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p id="index-39">The overall workflow for performing K-nearest neighbors classification using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> is as follows:</p>
<ol class="arabic simple">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function to split the data into a training and test set. Set the <code class="docutils literal notranslate"><span class="pre">stratify</span></code> argument to the class label column of the dataframe. Put the test set aside for now.</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> that specifies the preprocessing steps and the classifier.</p></li>
<li><p>Define the parameter grid by passing the set of <span class="math notranslate nohighlight">\(K\)</span> values that you would like to tune.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> to estimate the classifier accuracy for a range of <span class="math notranslate nohighlight">\(K\)</span> values. Pass the pipeline and parameter grid defined in steps 2. and 3. as the <code class="docutils literal notranslate"><span class="pre">param_grid</span></code> argument and the <code class="docutils literal notranslate"><span class="pre">estimator</span></code> argument, respectively.</p></li>
<li><p>Execute the grid search by passing the training data to the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method on the <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> instance created in step 4.</p></li>
<li><p>Pick a value of <span class="math notranslate nohighlight">\(K\)</span> that yields a high cross-validation accuracy estimate that doesn’t change much if you change <span class="math notranslate nohighlight">\(K\)</span> to a nearby value.</p></li>
<li><p>Create a new model object for the best parameter value (i.e., <span class="math notranslate nohighlight">\(K\)</span>), and retrain the classifier by calling the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method.</p></li>
<li><p>Evaluate the estimated accuracy of the classifier on the test set using the <code class="docutils literal notranslate"><span class="pre">score</span></code> method.</p></li>
</ol>
<p>In these last two chapters, we focused on the K-nearest neighbors algorithm,
but there are many other methods we could have used to predict a categorical label.
All algorithms have their strengths and weaknesses, and we summarize these for
the K-NN here.</p>
<p><strong>Strengths:</strong> K-nearest neighbors classification</p>
<ol class="arabic simple">
<li><p>is a simple, intuitive algorithm,</p></li>
<li><p>requires few assumptions about what the data must look like, and</p></li>
<li><p>works for binary (two-class) and multi-class (more than 2 classes) classification problems.</p></li>
</ol>
<p><strong>Weaknesses:</strong> K-nearest neighbors classification</p>
<ol class="arabic simple">
<li><p>becomes very slow as the training data gets larger,</p></li>
<li><p>may not perform well with a large number of predictors, and</p></li>
<li><p>may not perform well when classes are imbalanced.</p></li>
</ol>
</section>
<section id="predictor-variable-selection">
<h2><span class="section-number">6.8. </span>Predictor variable selection<a class="headerlink" href="#predictor-variable-selection" title="Permalink to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section is not required reading for the remainder of the textbook. It is included for those readers
interested in learning how irrelevant variables can influence the performance of a classifier, and how to
pick a subset of useful variables to include as predictors.</p>
</div>
<p id="index-40">Another potentially important part of tuning your classifier is to choose which
variables from your data will be treated as predictor variables. Technically, you can choose
anything from using a single predictor variable to using every variable in your
data; the K-nearest neighbors algorithm accepts any number of
predictors. However, it is <strong>not</strong> the case that using more predictors always
yields better predictions! In fact, sometimes including irrelevant predictors can
actually negatively affect classifier performance.</p>
<section id="the-effect-of-irrelevant-predictors">
<h3><span class="section-number">6.8.1. </span>The effect of irrelevant predictors<a class="headerlink" href="#the-effect-of-irrelevant-predictors" title="Permalink to this heading">#</a></h3>
<p>Let’s take a look at an example where K-nearest neighbors performs
worse when given more predictors to work with. In this example, we modified
the breast cancer data to have only the <code class="docutils literal notranslate"><span class="pre">Smoothness</span></code>, <code class="docutils literal notranslate"><span class="pre">Concavity</span></code>, and
<code class="docutils literal notranslate"><span class="pre">Perimeter</span></code> variables from the original data. Then, we added irrelevant
variables that we created ourselves using a random number generator.
The irrelevant variables each take a value of 0 or 1 with equal probability for each observation, regardless
of what the value <code class="docutils literal notranslate"><span class="pre">Class</span></code> variable takes. In other words, the irrelevant variables have
no meaningful relationship with the <code class="docutils literal notranslate"><span class="pre">Class</span></code> variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_irrelevant</span><span class="p">[</span>
    <span class="p">[</span><span class="s2">"Class"</span><span class="p">,</span> <span class="s2">"Smoothness"</span><span class="p">,</span> <span class="s2">"Concavity"</span><span class="p">,</span> <span class="s2">"Perimeter"</span><span class="p">,</span> <span class="s2">"Irrelevant1"</span><span class="p">,</span> <span class="s2">"Irrelevant2"</span><span class="p">]</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Class</th>
<th>Smoothness</th>
<th>Concavity</th>
<th>Perimeter</th>
<th>Irrelevant1</th>
<th>Irrelevant2</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>Malignant</td>
<td>0.11840</td>
<td>0.30010</td>
<td>122.80</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>1</th>
<td>Malignant</td>
<td>0.08474</td>
<td>0.08690</td>
<td>132.90</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>2</th>
<td>Malignant</td>
<td>0.10960</td>
<td>0.19740</td>
<td>130.00</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>Malignant</td>
<td>0.14250</td>
<td>0.24140</td>
<td>77.58</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>4</th>
<td>Malignant</td>
<td>0.10030</td>
<td>0.19800</td>
<td>135.10</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>...</th>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<th>564</th>
<td>Malignant</td>
<td>0.11100</td>
<td>0.24390</td>
<td>142.00</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>565</th>
<td>Malignant</td>
<td>0.09780</td>
<td>0.14400</td>
<td>131.20</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>566</th>
<td>Malignant</td>
<td>0.08455</td>
<td>0.09251</td>
<td>108.30</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<th>567</th>
<td>Malignant</td>
<td>0.11780</td>
<td>0.35140</td>
<td>140.10</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>568</th>
<td>Benign</td>
<td>0.05263</td>
<td>0.00000</td>
<td>47.92</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>569 rows × 6 columns</p>
</div></div></div>
</div>
<p>Next, we build a sequence of K-NN classifiers that include <code class="docutils literal notranslate"><span class="pre">Smoothness</span></code>,
<code class="docutils literal notranslate"><span class="pre">Concavity</span></code>, and <code class="docutils literal notranslate"><span class="pre">Perimeter</span></code> as predictor variables, but also increasingly many irrelevant
variables. In particular, we create 6 data sets with 0, 5, 10, 15, 20, and 40 irrelevant predictors.
Then we build a model, tuned via 5-fold cross-validation, for each data set.
<a class="reference internal" href="#fig-06-performance-irrelevant-features"><span class="std std-numref">Fig. 6.9</span></a> shows
the estimated cross-validation accuracy versus the number of irrelevant predictors.  As
we add more irrelevant predictor variables, the estimated accuracy of our
classifier decreases. This is because the irrelevant variables add a random
amount to the distance between each pair of observations; the more irrelevant
variables there are, the more (random) influence they have, and the more they
corrupt the set of nearest neighbors that vote on the class of the new
observation to predict.</p>
<figure class="align-default" id="fig-06-performance-irrelevant-features">
<div class="output text_html">
<style>
  <ins>#altair-viz-94d7e330b7d949e0a2aea74a291d1f01.vega-embed</ins><del>#altair-viz-da77f368171c415d9e942b095dd925b9.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-94d7e330b7d949e0a2aea74a291d1f01.vega-embed</ins><del>#altair-viz-da77f368171c415d9e942b095dd925b9.vega-embed</del> details,
  <ins>#altair-viz-94d7e330b7d949e0a2aea74a291d1f01.vega-embed</ins><del>#altair-viz-da77f368171c415d9e942b095dd925b9.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-94d7e330b7d949e0a2aea74a291d1f01"><div id="altair-viz-da77f368171c415d9e942b095dd925b9"></div><img src="prerendered/fig-06-performance-irrelevant-features.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.9 </span><span class="caption-text">Effect of inclusion of irrelevant predictors.</span><a class="headerlink" href="#fig-06-performance-irrelevant-features" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
<p>Although the accuracy decreases as expected, one surprising thing about
<a class="reference internal" href="#fig-06-performance-irrelevant-features"><span class="std std-numref">Fig. 6.9</span></a> is that it shows that the method
still outperforms the baseline majority classifier (with about <span class="pasted-text">63</span>% accuracy)
even with 40 irrelevant variables.
How could that be? <a class="reference internal" href="#fig-06-neighbors-irrelevant-features"><span class="std std-numref">Fig. 6.10</span></a> provides the answer:
the tuning procedure for the K-nearest neighbors classifier combats the extra randomness from the irrelevant variables
by increasing the number of neighbors. Of course, because of all the extra noise in the data from the irrelevant
variables, the number of neighbors does not increase smoothly; but the general trend is increasing. <a class="reference internal" href="#fig-06-fixed-irrelevant-features"><span class="std std-numref">Fig. 6.11</span></a> corroborates
this evidence; if we fix the number of neighbors to <span class="math notranslate nohighlight">\(K=3\)</span>, the accuracy falls off more quickly.</p>
<figure class="align-default" id="fig-06-neighbors-irrelevant-features">
<div class="output text_html">
<style>
  <ins>#altair-viz-c1c2c5acc3164e0aa355a8ad17396d7b.vega-embed</ins><del>#altair-viz-16b8702f076f4fcf8a0c6729aaa61df6.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-c1c2c5acc3164e0aa355a8ad17396d7b.vega-embed</ins><del>#altair-viz-16b8702f076f4fcf8a0c6729aaa61df6.vega-embed</del> details,
  <ins>#altair-viz-c1c2c5acc3164e0aa355a8ad17396d7b.vega-embed</ins><del>#altair-viz-16b8702f076f4fcf8a0c6729aaa61df6.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-c1c2c5acc3164e0aa355a8ad17396d7b"><div id="altair-viz-16b8702f076f4fcf8a0c6729aaa61df6"></div><img src="prerendered/fig-06-neighbors-irrelevant-features.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.10 </span><span class="caption-text">Tuned number of neighbors for varying number of irrelevant predictors.</span><a class="headerlink" href="#fig-06-neighbors-irrelevant-features" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
<figure class="align-default" id="fig-06-fixed-irrelevant-features">
<div class="output text_html">
<style>
  <ins>#altair-viz-8479c86f8d8245bca09d5cb384740ab9.vega-embed</ins><del>#altair-viz-079587c88d0c4e2aaba1f3d98f5efec6.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-8479c86f8d8245bca09d5cb384740ab9.vega-embed</ins><del>#altair-viz-079587c88d0c4e2aaba1f3d98f5efec6.vega-embed</del> details,
  <ins>#altair-viz-8479c86f8d8245bca09d5cb384740ab9.vega-embed</ins><del>#altair-viz-079587c88d0c4e2aaba1f3d98f5efec6.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-8479c86f8d8245bca09d5cb384740ab9"><div id="altair-viz-079587c88d0c4e2aaba1f3d98f5efec6"></div><img src="prerendered/fig-06-fixed-irrelevant-features.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.11 </span><span class="caption-text">Accuracy versus number of irrelevant predictors for tuned and untuned number of neighbors.</span><a class="headerlink" href="#fig-06-fixed-irrelevant-features" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
</section>
<section id="finding-a-good-subset-of-predictors">
<h3><span class="section-number">6.8.2. </span>Finding a good subset of predictors<a class="headerlink" href="#finding-a-good-subset-of-predictors" title="Permalink to this heading">#</a></h3>
<p>So then, if it is not ideal to use all of our variables as predictors without consideration, how
do we choose which variables we <em>should</em> use?  A simple method is to rely on your scientific understanding
of the data to tell you which variables are not likely to be useful predictors. For example, in the cancer
data that we have been studying, the <code class="docutils literal notranslate"><span class="pre">ID</span></code> variable is just a unique identifier for the observation.
As it is not related to any measured property of the cells, the <code class="docutils literal notranslate"><span class="pre">ID</span></code> variable should therefore not be used
as a predictor. That is, of course, a very clear-cut case. But the decision for the remaining variables
is less obvious, as all seem like reasonable candidates. It
is not clear which subset of them will create the best classifier. One could use visualizations and
other exploratory analyses to try to help understand which variables are potentially relevant, but
this process is both time-consuming and error-prone when there are many variables to consider.
Therefore we need a more systematic and programmatic way of choosing variables.
This is a very difficult problem to solve in
general, and there are a number of methods that have been developed that apply
in particular cases of interest. Here we will discuss two basic
selection methods as an introduction to the topic. See the additional resources at the end of
this chapter to find out where you can learn more about variable selection, including more advanced methods.</p>
<span class="target" id="index-41"></span><p id="index-42">The first idea you might think of for a systematic way to select predictors
is to try all possible subsets of predictors and then pick the set that results in the “best” classifier.
This procedure is indeed a well-known variable selection method referred to
as <em>best subset selection</em> <span id="id2">[<a class="reference internal" href="#id13" title="Evelyn Martin Lansdowne Beale, Maurice George Kendall, and David Mann. The discarding of variables in multivariate analysis. Biometrika, 54(3-4):357–366, 1967.">Beale <em>et al.</em>, 1967</a>, <a class="reference internal" href="#id14" title="Ronald Hocking and R. N. Leslie. Selection of the best subset in regression analysis. Technometrics, 9(4):531–540, 1967.">Hocking and Leslie, 1967</a>]</span>.
In particular, you</p>
<ol class="arabic simple">
<li><p>create a separate model for every possible subset of predictors,</p></li>
<li><p>tune each one using cross-validation, and</p></li>
<li><p>pick the subset of predictors that gives you the highest cross-validation accuracy.</p></li>
</ol>
<p>Best subset selection is applicable to any classification method (K-NN or otherwise).
However, it becomes very slow when you have even a moderate
number of predictors to choose from (say, around 10). This is because the number of possible predictor subsets
grows very quickly with the number of predictors, and you have to train the model (itself
a slow process!) for each one. For example, if we have 2 predictors—let’s call
them A and B—then we have 3 variable sets to try: A alone, B alone, and finally A
and B together. If we have 3 predictors—A, B, and C—then we have 7
to try: A, B, C, AB, BC, AC, and ABC. In general, the number of models
we have to train for <span class="math notranslate nohighlight">\(m\)</span> predictors is <span class="math notranslate nohighlight">\(2^m-1\)</span>; in other words, when we
get to 10 predictors we have over <em>one thousand</em> models to train, and
at 20 predictors we have over <em>one million</em> models to train!
So although it is a simple method, best subset selection is usually too computationally
expensive to use in practice.</p>
<p id="index-43">Another idea is to iteratively build up a model by adding one predictor variable
at a time. This method—known as <em>forward selection</em> <span id="id3">[<a class="reference internal" href="#id12" title="Norman Draper and Harry Smith. Applied Regression Analysis. Wiley, 1966.">Draper and Smith, 1966</a>, <a class="reference internal" href="#id7" title="M. Eforymson. Stepwise regression—a backward and forward look. In Eastern Regional Meetings of the Institute of Mathematical Statistics. 1966.">Eforymson, 1966</a>]</span>—is also widely
applicable and fairly straightforward. It involves the following steps:</p>
<ol class="arabic simple">
<li><p>Start with a model having no predictors.</p></li>
<li><p>Run the following 3 steps until you run out of predictors:</p>
<ol class="arabic simple">
<li><p>For each unused predictor, add it to the model to form a <em>candidate model</em>.</p></li>
<li><p>Tune all of the candidate models.</p></li>
<li><p>Update the model to be the candidate model with the highest cross-validation accuracy.</p></li>
</ol>
</li>
<li><p>Select the model that provides the best trade-off between accuracy and simplicity.</p></li>
</ol>
<p>Say you have <span class="math notranslate nohighlight">\(m\)</span> total predictors to work with. In the first iteration, you have to make
<span class="math notranslate nohighlight">\(m\)</span> candidate models, each with 1 predictor. Then in the second iteration, you have
to make <span class="math notranslate nohighlight">\(m-1\)</span> candidate models, each with 2 predictors (the one you chose before and a new one).
This pattern continues for as many iterations as you want. If you run the method
all the way until you run out of predictors to choose, you will end up training
<span class="math notranslate nohighlight">\(\frac{1}{2}m(m+1)\)</span> separate models. This is a <em>big</em> improvement from the <span class="math notranslate nohighlight">\(2^m-1\)</span>
models that best subset selection requires you to train! For example, while best subset selection requires
training over 1000 candidate models with 10 predictors, forward selection requires training only 55 candidate models.
Therefore we will continue the rest of this section using forward selection.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One word of caution before we move on. Every additional model that you train
increases the likelihood that you will get unlucky and stumble
on a model that has a high cross-validation accuracy estimate, but a low true
accuracy on the test data and other future observations.
Since forward selection involves training a lot of models, you run a fairly
high risk of this happening. To keep this risk low, only use forward selection
when you have a large amount of data and a relatively small total number of
predictors. More advanced methods do not suffer from this
problem as much; see the additional resources at the end of this chapter for
where to learn more about advanced predictor selection methods.</p>
</div>
</section>
<section id="forward-selection-in-python">
<h3><span class="section-number">6.8.3. </span>Forward selection in Python<a class="headerlink" href="#forward-selection-in-python" title="Permalink to this heading">#</a></h3>
<p id="index-44">We now turn to implementing forward selection in Python.
First we will extract a smaller set of predictors to work with in this illustrative example—<code class="docutils literal notranslate"><span class="pre">Smoothness</span></code>,
<code class="docutils literal notranslate"><span class="pre">Concavity</span></code>, <code class="docutils literal notranslate"><span class="pre">Perimeter</span></code>, <code class="docutils literal notranslate"><span class="pre">Irrelevant1</span></code>, <code class="docutils literal notranslate"><span class="pre">Irrelevant2</span></code>, and <code class="docutils literal notranslate"><span class="pre">Irrelevant3</span></code>—as well as the <code class="docutils literal notranslate"><span class="pre">Class</span></code> variable as the label.
We will also extract the column names for the full set of predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer_subset</span> <span class="o">=</span> <span class="n">cancer_irrelevant</span><span class="p">[</span>
    <span class="p">[</span>
        <span class="s2">"Class"</span><span class="p">,</span>
        <span class="s2">"Smoothness"</span><span class="p">,</span>
        <span class="s2">"Concavity"</span><span class="p">,</span>
        <span class="s2">"Perimeter"</span><span class="p">,</span>
        <span class="s2">"Irrelevant1"</span><span class="p">,</span>
        <span class="s2">"Irrelevant2"</span><span class="p">,</span>
        <span class="s2">"Irrelevant3"</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">]</span>

<span class="n">names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cancer_subset</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">cancer_subset</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>Class</th>
<th>Smoothness</th>
<th>Concavity</th>
<th>Perimeter</th>
<th>Irrelevant1</th>
<th>Irrelevant2</th>
<th>Irrelevant3</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>Malignant</td>
<td>0.11840</td>
<td>0.30010</td>
<td>122.80</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>Malignant</td>
<td>0.08474</td>
<td>0.08690</td>
<td>132.90</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>2</th>
<td>Malignant</td>
<td>0.10960</td>
<td>0.19740</td>
<td>130.00</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>Malignant</td>
<td>0.14250</td>
<td>0.24140</td>
<td>77.58</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>4</th>
<td>Malignant</td>
<td>0.10030</td>
<td>0.19800</td>
<td>135.10</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>...</th>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr>
<th>564</th>
<td>Malignant</td>
<td>0.11100</td>
<td>0.24390</td>
<td>142.00</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>565</th>
<td>Malignant</td>
<td>0.09780</td>
<td>0.14400</td>
<td>131.20</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>566</th>
<td>Malignant</td>
<td>0.08455</td>
<td>0.09251</td>
<td>108.30</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>567</th>
<td>Malignant</td>
<td>0.11780</td>
<td>0.35140</td>
<td>140.10</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>568</th>
<td>Benign</td>
<td>0.05263</td>
<td>0.00000</td>
<td>47.92</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>569 rows × 7 columns</p>
</div></div></div>
</div>
<p>To perform forward selection, we could use the
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html"><code class="docutils literal notranslate"><span class="pre">SequentialFeatureSelector</span></code></a>
from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>; but it is difficult to combine this approach with parameter tuning to find a good number of neighbors
for each set of features. Instead we will code the forward selection algorithm manually.
In particular, we need code that tries adding each available predictor to a model, finding the best, and iterating.
If you recall the end of the wrangling chapter, we mentioned
that sometimes one needs more flexible forms of iteration than what
we have used earlier, and in these cases one typically resorts to
a <em>for loop</em>; see
the <a class="reference external" href="https://wesmckinney.com/book/python-basics.html#control_for">control flow section</a> in
<em>Python for Data Analysis</em> <span id="id4">[<a class="reference internal link-to-diff" href="wrangling.html#id52" title="Wes McKinney. Python for data analysis: Data wrangling with Pandas, NumPy, and IPython. &quot; O'Reilly Media, Inc.&quot;, 2012.">McKinney, 2012</a>]</span>.
Here we will use two for loops: one over increasing predictor set sizes
(where you see <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">i</span> <span class="pre">in</span> <span class="pre">range(1,</span> <span class="pre">n_total</span> <span class="pre">+</span> <span class="pre">1):</span></code> below),
and another to check which predictor to add in each round (where you see <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">j</span> <span class="pre">in</span> <span class="pre">range(len(names))</span></code> below).
For each set of predictors to try, we extract the subset of predictors,
pass it into a preprocessor, build a <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> that tunes
a K-NN classifier using 10-fold cross-validation,
and finally records the estimated accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">make_column_selector</span>

<span class="n">accuracy_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"size"</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">"selected_predictors"</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">"accuracy"</span><span class="p">:</span> <span class="p">[]}</span>

<span class="c1"># store the total number of predictors</span>
<span class="n">n_total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

<span class="c1"># start with an empty list of selected predictors</span>
<span class="n">selected</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># create the pipeline and CV grid search objects</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"kneighborsclassifier__n_neighbors"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">cancer_preprocessor</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">(</span>
    <span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">make_column_selector</span><span class="p">(</span><span class="n">dtype_include</span><span class="o">=</span><span class="s2">"number"</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">cancer_tune_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">cancer_preprocessor</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">())</span>
<span class="n">cancer_tune_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">cancer_tune_pipe</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># for every possible number of predictors</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_total</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>
    <span class="c1"># for every possible predictor to add</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">)):</span>
        <span class="c1"># Add remaining predictor j to the model</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">cancer_subset</span><span class="p">[</span><span class="n">selected</span> <span class="o">+</span> <span class="p">[</span><span class="n">names</span><span class="p">[</span><span class="n">j</span><span class="p">]]]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">cancer_subset</span><span class="p">[</span><span class="s2">"Class"</span><span class="p">]</span>

        <span class="c1"># Find the best K for this set of predictors</span>
        <span class="n">cancer_tune_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">accuracies_grid</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cancer_tune_grid</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>

        <span class="c1"># Store the tuned accuracy for this set of predictors</span>
        <span class="n">accs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracies_grid</span><span class="p">[</span><span class="s2">"mean_test_score"</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

    <span class="c1"># get the best new set of predictors that maximize cv accuracy</span>
    <span class="n">best_set</span> <span class="o">=</span> <span class="n">selected</span> <span class="o">+</span> <span class="p">[</span><span class="n">names</span><span class="p">[</span><span class="n">accs</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]]</span>

    <span class="c1"># store the results for this round of forward selection</span>
    <span class="n">accuracy_dict</span><span class="p">[</span><span class="s2">"size"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">accuracy_dict</span><span class="p">[</span><span class="s2">"selected_predictors"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">", "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">best_set</span><span class="p">))</span>
    <span class="n">accuracy_dict</span><span class="p">[</span><span class="s2">"accuracy"</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accs</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>

    <span class="c1"># update the selected &amp; available sets of predictors</span>
    <span class="n">selected</span> <span class="o">=</span> <span class="n">best_set</span>
    <span class="k">del</span> <span class="n">names</span><span class="p">[</span><span class="n">accs</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>

<span class="n">accuracies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">accuracy_dict</span><span class="p">)</span>
<span class="n">accuracies</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>size</th>
<th>selected_predictors</th>
<th>accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>1</td>
<td>Perimeter</td>
<td>0.891103</td>
</tr>
<tr>
<th>1</th>
<td>2</td>
<td>Perimeter, Concavity</td>
<td>0.917450</td>
</tr>
<tr>
<th>2</th>
<td>3</td>
<td>Perimeter, Concavity, Smoothness</td>
<td>0.931454</td>
</tr>
<tr>
<th>3</th>
<td>4</td>
<td>Perimeter, Concavity, Smoothness, Irrelevant1</td>
<td>0.926253</td>
</tr>
<tr>
<th>4</th>
<td>5</td>
<td>Perimeter, Concavity, Smoothness, Irrelevant1,...</td>
<td>0.926253</td>
</tr>
<tr>
<th>5</th>
<td>6</td>
<td>Perimeter, Concavity, Smoothness, Irrelevant1,...</td>
<td>0.906955</td>
</tr>
</tbody>
</table>
</div></div></div>
</div>
<p id="index-45">Interesting! The forward selection procedure first added the three meaningful variables <code class="docutils literal notranslate"><span class="pre">Perimeter</span></code>,
<code class="docutils literal notranslate"><span class="pre">Concavity</span></code>, and <code class="docutils literal notranslate"><span class="pre">Smoothness</span></code>, followed by the irrelevant variables. <a class="reference internal" href="#fig-06-fwdsel-3"><span class="std std-numref">Fig. 6.12</span></a>
visualizes the accuracy versus the number of predictors in the model. You can see that
as meaningful predictors are added, the estimated accuracy increases substantially; and as you add irrelevant
variables, the accuracy either exhibits small fluctuations or decreases as the model attempts to tune the number
of neighbors to account for the extra noise. In order to pick the right model from the sequence, you have
to balance high accuracy and model simplicity (i.e., having fewer predictors and a lower chance of overfitting).
The way to find that balance is to look for the <em>elbow</em>
in <a class="reference internal" href="#fig-06-fwdsel-3"><span class="std std-numref">Fig. 6.12</span></a>, i.e., the place on the plot where the accuracy stops increasing dramatically and
levels off or begins to decrease. The elbow in <a class="reference internal" href="#fig-06-fwdsel-3"><span class="std std-numref">Fig. 6.12</span></a> appears to occur at the model with
3 predictors; after that point the accuracy levels off. So here the right trade-off of accuracy and number of predictors
occurs with 3 variables: <code class="docutils literal notranslate"><span class="pre">Perimeter,</span> <span class="pre">Concavity,</span> <span class="pre">Smoothness</span></code>. In other words, we have successfully removed irrelevant
predictors from the model! It is always worth remembering, however, that what cross-validation gives you
is an <em>estimate</em> of the true accuracy; you have to use your judgement when looking at this plot to decide
where the elbow occurs, and whether adding a variable provides a meaningful increase in accuracy.</p>
<figure class="align-default" id="fig-06-fwdsel-3">
<div class="output text_html">
<style>
  <ins>#altair-viz-072148e092294f3785ca654c21daba84.vega-embed</ins><del>#altair-viz-4ed40df63bca411a808f97b4fcfafd83.vega-embed</del> {
    width: 100%;
    display: flex;
  }

  <ins>#altair-viz-072148e092294f3785ca654c21daba84.vega-embed</ins><del>#altair-viz-4ed40df63bca411a808f97b4fcfafd83.vega-embed</del> details,
  <ins>#altair-viz-072148e092294f3785ca654c21daba84.vega-embed</ins><del>#altair-viz-4ed40df63bca411a808f97b4fcfafd83.vega-embed</del> details summary {
    position: relative;
  }
</style>
<div id="altair-viz-072148e092294f3785ca654c21daba84"><div id="altair-viz-4ed40df63bca411a808f97b4fcfafd83"></div><img src="prerendered/fig-06-fwdsel-3.png"/>
</div><figcaption>
<p><span class="caption-number">Fig. 6.12 </span><span class="caption-text">Estimated accuracy versus the number of predictors for the sequence of models built using forward selection.</span><a class="headerlink" href="#fig-06-fwdsel-3" title="Permalink to this image">#</a></p>
</figcaption>
</div></figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since the choice of which variables to include as predictors is
part of tuning your classifier, you <em>cannot use your test data</em> for this
process!</p>
</div>
</section>
</section>
<section id="exercises">
<h2><span class="section-number">6.9. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<p>Practice exercises for the material covered in this chapter<ins class="diff"> </ins>can be found in the<ins class="diff">accompanying</ins> <del class="diff">accompanying</del><a class="reference external" href="https://worksheets.python.datasciencebook.ca">worksheets repository</a><ins class="diff"> in</ins>
<del class="diff">in </del>the “Classification II: evaluation and tuning” row.<ins class="diff"> </ins>You can <ins class="diff">preview</ins><del class="diff">launch</del> <ins class="diff">anon-interactive</ins><del class="diff">an</del> <del class="diff">interactive </del>version of the worksheet <ins class="diff">for</ins><del class="diff">in</del> <ins class="diff">this</ins><del class="diff">your</del> <ins class="diff">chapter</ins><del class="diff">browser</del> by clicking <ins class="diff">“view</ins><del class="diff">the “launch binder” button.</del>
<ins class="diff">worksheet.”</ins><del class="diff">You</del> <ins class="diff">To</ins><del class="diff">can</del> <ins class="diff">work</ins><del class="diff">also</del> <ins class="diff">on</ins><del class="diff">preview</del> <ins class="diff">the</ins><del class="diff">a</del> <ins class="diff">exercises</ins><del class="diff">non-interactive</del> <ins class="diff">interactively,</ins><del class="diff">version</del> <ins class="diff">follow</ins><del class="diff">of</del> the <ins class="diff">instructions</ins><del class="diff">worksheet</del> <ins class="diff">in</ins><del class="diff">by clicking “view worksheet.”</del>
<ins class="diff">the</ins><del class="diff">If</del> <ins class="diff">worksheets</ins><del class="diff">you</del> <ins class="diff">repository</ins><del class="diff">instead</del> <del class="diff">decide </del>to download <ins class="diff">all</ins><del class="diff">the</del> <ins class="diff">worksheets,</ins><del class="diff">worksheet</del> and <del class="diff">run it on your own machine,make sure to </del>follow the<del class="diff"> </del>instructions for computer setup<ins class="diff"> </ins>found in <a class="reference internal link-to-diff" href="setup.html#move-to-your-own-machine"><span class="std std-numref">Chapter 13</span></a>. This will ensure<del class="diff"> </del>that the automated feedback<ins class="diff"> </ins>and guidance that the worksheets provide will<del class="diff"> </del>function as intended.</p>
</section>
<section id="additional-resources">
<h2><span class="section-number">6.10. </span>Additional resources<a class="headerlink" href="#additional-resources" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>The <a class="reference external" href="https://scikit-learn.org/stable/"><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> website</a> is an excellent
reference for more details on, and advanced usage of, the functions and
packages in the past two chapters. Aside from that, it also offers many
useful <a class="reference external" href="https://scikit-learn.org/stable/tutorial/index.html">tutorials</a>
to get you started. It’s worth noting that the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> package
does a lot more than just classification, and so the
examples on the website similarly go beyond classification as well. In the next
two chapters, you’ll learn about another kind of predictive modeling setting,
so it might be worth visiting the website only after reading through those
chapters.</p></li>
<li><p><a class="reference external" href="https://www.statlearning.com/"><em>An Introduction to Statistical Learning</em></a> <span id="id5">[<a class="reference internal link-to-diff" href="regression2.html#id21" title="Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An Introduction to Statistical Learning. Springer, 1st edition, 2013. URL: https://www.statlearning.com/.">James <em>et al.</em>, 2013</a>]</span> provides
a great next stop in the process of
learning about classification. Chapter 4 discusses additional basic techniques
for classification that we do not cover, such as logistic regression, linear
discriminant analysis, and naive Bayes. Chapter 5 goes into much more detail
about cross-validation. Chapters 8 and 9 cover decision trees and support
vector machines, two very popular but more advanced classification methods.
Finally, Chapter 6 covers a number of methods for selecting predictor
variables. Note that while this book is still a very accessible introductory
text, it requires a bit more mathematical background than we require.</p></li>
</ul>
</section>
<section id="references">
<h2><span class="section-number">6.11. </span>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<div class="docutils container" id="id6">
<dl class="citation">
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id2">BKM67</a></span></dt>
<dd><p>Evelyn Martin Lansdowne Beale, Maurice George Kendall, and David Mann. The discarding of variables in multivariate analysis. <em>Biometrika</em>, 54(3-4):357–366, 1967.</p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id3">DS66</a></span></dt>
<dd><p>Norman Draper and Harry Smith. <em>Applied Regression Analysis</em>. Wiley, 1966.</p>
</dd>
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id3">Efo66</a></span></dt>
<dd><p>M. Eforymson. Stepwise regression—a backward and forward look. In <em>Eastern Regional Meetings of the Institute of Mathematical Statistics</em>. 1966.</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id2">HL67</a></span></dt>
<dd><p>Ronald Hocking and R. N. Leslie. Selection of the best subset in regression analysis. <em>Technometrics</em>, 9(4):531–540, 1967.</p>
</dd>
<dt class="label" id="id25"><span class="brackets"><a class="fn-backref" href="#id5">JWHT13</a></span></dt>
<dd><p>Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. <em>An Introduction to Statistical Learning</em>. Springer, 1st edition, 2013. URL: <a class="reference external" href="https://www.statlearning.com/">https://www.statlearning.com/</a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id4">McK12</a></span></dt>
<dd><p>Wes McKinney. <em>Python for data analysis: Data wrangling with Pandas, NumPy, and IPython</em>. " O'Reilly Media, Inc.", 2012.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id1">SWM93</a></span></dt>
<dd><p>William Nick Street, William Wolberg, and Olvi Mangasarian. Nuclear feature extraction for breast tumor diagnosis. In <em>International Symposium on Electronic Imaging: Science and Technology</em>. 1993.</p>
</dd>
</dl>
</div>
</section>
</section>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</article>
<footer class="prev-next-footer">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev link-to-diff" href="classification1.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title"><span class="section-number">5. </span>Classification I: training &amp; predicting</p>
</div>
</a>
<a class="right-next link-to-diff" href="regression1.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title"><span class="section-number">7. </span>Regression I: K-nearest neighbors</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">6.1. Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-learning-objectives">6.2. Chapter learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-performance">6.3. Evaluating performance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#randomness-and-seeds">6.4. Randomness and seeds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-performance-with-scikit-learn">6.5. Evaluating performance with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-train-test-split">6.5.1. Create the train / test split</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocess-the-data">6.5.2. Preprocess the data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-classifier">6.5.3. Train the classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predict-the-labels-in-the-test-set">6.5.4. Predict the labels in the test set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-performance">6.5.5. Evaluate performance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#critically-analyze-performance">6.5.6. Critically analyze performance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-the-classifier">6.6. Tuning the classifier</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">6.6.1. Cross-validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-value-selection">6.6.2. Parameter value selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#under-overfitting">6.6.3. Under/Overfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-on-the-test-set">6.6.4. Evaluating on the test set</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">6.7. Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictor-variable-selection">6.8. Predictor variable selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-effect-of-irrelevant-predictors">6.8.1. The effect of irrelevant predictors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-a-good-subset-of-predictors">6.8.2. Finding a good subset of predictors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-selection-in-python">6.8.3. Forward selection in Python</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">6.9. Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-resources">6.10. Additional resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">6.11. References</a></li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner container">
<div class="footer-item">
<p class="component-author">
By Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom, and Lindsey Heagy
</p>
</div>
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2022.
      <br/>
</p>
</div>
<div class="footer-item">
</div>
<div class="footer-item">
</div>
</div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>
<footer class="bd-footer">
</footer>
</body>
</html>