{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd31d4e9",
   "metadata": {},
   "source": [
    "# Python and the Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f201704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:11.833455Z",
     "iopub.status.busy": "2022-06-08T02:12:11.833023Z",
     "iopub.status.idle": "2022-06-08T02:12:12.303551Z",
     "shell.execute_reply": "2022-06-08T02:12:12.303118Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097afa8",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "his chapter provides an introduction to data science and the Python programming language.\n",
    "The goal here is to get your hands dirty right from the start! We will walk through an entire data analysis,\n",
    "and along the way introduce different types of data analysis question, some fundamental programming \n",
    "concepts in Python, and the basics of loading, cleaning, and visualizing data. In the following chapters, we will\n",
    "dig into each of these steps in much more detail; but for now, let's jump in to see how much we can do \n",
    "with data science!\n",
    "\n",
    "## Chapter learning objectives\n",
    "\n",
    "By the end of the chapter, readers will be able to do the following:\n",
    "\n",
    "- Identify the different types of data analysis question and categorize a question into the correct type.\n",
    "- Load the `pandas` package into Python.\n",
    "- Read tabular data with `read_csv`.\n",
    "- Use `help()` to access help and documentation tools in Python.\n",
    "- Create new variables and objects in Python.\n",
    "- Do indexing and slicing with `.loc[]` and `.iloc[]` properties.\n",
    "- Select columns of a dataframe using df[] notation.\n",
    "- Visualize data with an `altair` bar plot.\n",
    "\n",
    "## Canadian languages data set\n",
    "\n",
    "In this chapter, \\index{Canadian languages} we will walk through a full analysis of a data set relating to\n",
    "languages spoken at home by Canadian residents. Many Indigenous peoples exist in Canada \n",
    "with their own cultures and languages; these languages are often unique to Canada and not spoken\n",
    "anywhere else in the world [@statcan2018mothertongue]. Sadly, colonization has\n",
    "led to the loss of many of these languages. For instance, generations of\n",
    "children were not allowed to speak their mother tongue (the first language an\n",
    "individual learns in childhood) in Canadian residential schools. Colonizers\n",
    "also renamed places they had \"discovered\" [@wilson2018].  Acts such as these\n",
    "have significantly harmed the continuity of Indigenous languages in Canada, and\n",
    "some languages are considered \"endangered\" as few people report speaking them. \n",
    "To learn more, please see *Canadian Geographic*'s article, \"Mapping Indigenous Languages in \n",
    "Canada\" [@walker2017], \n",
    "*They Came for the Children: Canada, Aboriginal \n",
    "peoples, and Residential Schools* [@children2012] \n",
    "and the *Truth and Reconciliation Commission of Canada's* \n",
    "*Calls to Action* [@calls2015].\n",
    "\n",
    "The data set we will study in this chapter is taken from \n",
    "[the `canlang` R data package](https://ttimbers.github.io/canlang/) \n",
    "[@timbers2020canlang], which has\n",
    "population language data collected during the 2016 Canadian census [@cancensus2016]. \n",
    "In this data, there are 214 languages recorded, each having six different properties:\n",
    "\n",
    "1. `category`: Higher-level language category, describing whether the language is an Official Canadian language, an Aboriginal (i.e., Indigenous) language, or a Non-Official and Non-Aboriginal language.\n",
    "2. `language`: The name of the language.\n",
    "3. `mother_tongue`: Number of Canadian residents who reported the language as their mother tongue. Mother tongue is generally defined as the language someone was exposed to since birth.\n",
    "4. `most_at_home`: Number of Canadian residents who reported the language as being spoken most often at home.\n",
    "5. `most_at_work`: Number of Canadian residents who reported the language as being used most often at work.\n",
    "6. `lang_known`: Number of Canadian residents who reported knowledge of the language.\n",
    "\n",
    "According to the census, more than 60 Aboriginal languages were reported\n",
    "as being spoken in Canada. Suppose we want to know which are the most common;\n",
    "then we might ask the following question, which we wish to answer using our data: \n",
    "\n",
    "*Which ten Aboriginal languages were most often reported in 2016 as mother\n",
    "tongues in Canada, and how many people speak each of them?* \n",
    "\n",
    "> **Note:** Data science\\index{data science!good practices} cannot be done without \n",
    "> a deep understanding of the data and\n",
    "> problem domain. In this book, we have simplified the data sets used in our\n",
    "> examples to concentrate on methods and fundamental concepts. But in real\n",
    "> life, you cannot and should not do data science without a domain expert.\n",
    "> Alternatively, it is common to practice data science in your own domain of\n",
    "> expertise! Remember that when you work with data, it is essential to think\n",
    "> about *how* the data were collected, which affects the conclusions you can\n",
    "> draw. If your data are biased, then your results will be biased!\n",
    "\n",
    "## Asking a question \n",
    "\n",
    "Every good data analysis begins with a *question*&mdash;like the\n",
    "above&mdash;that you aim to answer using data. As it turns out, there\n",
    "are actually a number of different *types* of question regarding data:\n",
    "descriptive, exploratory, inferential, predictive, causal, and mechanistic,\n",
    "all of which are defined in {numref}`questions-table`. [@leek2015question; @peng2015art]\n",
    "Carefully formulating a question as early as possible in your analysis&mdash;and \n",
    "correctly identifying which type of question it is&mdash;will guide your overall approach to \n",
    "the analysis as well as the selection of appropriate tools.\\index{question!data analysis}\n",
    "\\index{descriptive question!definition}\n",
    "\\index{exploratory question!definition}\n",
    "\\index{predictive question!definition}\n",
    "\\index{inferential question!definition}\n",
    "\\index{causal question!definition}\n",
    "\\index{mechanistic question!definition}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{list-table} Types of data analysis question.\n",
    ":header-rows: 1\n",
    ":name: questions-table\n",
    "\n",
    "* - Question type\n",
    "  - Description\n",
    "  - Example\n",
    "* - Descriptive\n",
    "  - A question that asks about summarized characteristics of a data set without interpretation (i.e., report a fact).\n",
    "  - How many people live in each province and territory in Canada?\n",
    "* - Exploratory\n",
    "  - A question that asks if there are patterns, trends, or relationships within a single data set. Often used to propose hypotheses for future study.\n",
    "  - Does political party voting change with indicators of wealth in a set of data collected on 2,000 people living in Canada?\n",
    "* - Predictive\n",
    "  - A question that asks about predicting measurements or labels for individuals (people or things). The focus is on what things predict some outcome, but not what causes the outcome.\n",
    "  - What political party will someone vote for in the next Canadian election?\n",
    "* - Inferential\n",
    "  - A question that looks for patterns, trends, or relationships in a single data set **and** also asks for quantification of how applicable these findings are to the wider population.\n",
    "  - Does political party voting change with indicators of wealth for all people living in Canada?\n",
    "* - Causal\n",
    "  - A question that asks about whether changing one factor will lead to a change in another factor, on average, in the wider population.\n",
    "  - Does wealth lead to voting for a certain political party in Canadian elections?\n",
    "* - Mechanistic\n",
    "  - A question that asks about the underlying mechanism of the observed patterns, trends, or relationships (i.e., how does it happen?)\n",
    "  - How does wealth lead to voting for a certain political party in Canadian elections?\n",
    "  \n",
    "```\n",
    "\n",
    "\n",
    "In this book, you will learn techniques to answer the \n",
    "first four types of question: descriptive, exploratory, predictive, and inferential; \n",
    "causal and mechanistic questions are beyond the scope of this book.\n",
    "In particular, you will learn how to apply the following analysis tools:\n",
    "\n",
    "1. **Summarization:** \\index{summarization!overview} computing and reporting aggregated values pertaining to a data set. \n",
    "Summarization is most often used to answer descriptive questions,\n",
    "and can occasionally help with answering exploratory questions.\n",
    "For example, you might use summarization to answer the following question: \n",
    "*What is the average race time for runners in this data set?*\n",
    "Tools for summarization are covered in detail in Chapters \\@ref(reading)\n",
    "and \\@ref(wrangling), but appear regularly throughout the text.\n",
    "2. **Visualization:** \\index{visualization!overview} plotting data graphically. \n",
    "Visualization is typically used to answer descriptive and exploratory questions,\n",
    "but plays a critical supporting role in answering all of the types of question in {numref}`questions-table`.\n",
    "For example, you might use visualization to answer the following question:\n",
    "*Is there any relationship between race time and age for runners in this data set?* \n",
    "This is covered in detail in Chapter \\@ref(viz), but again appears regularly throughout the book.\n",
    "3. **Classification:** \\index{classification!overview} predicting a class or category for a new observation.\n",
    "Classification is used to answer predictive questions.\n",
    "For example, you might use classification to answer the following question:\n",
    "*Given measurements of a tumor's average cell area and perimeter, is the tumor benign or malignant?*\n",
    "Classification is covered in Chapters \\@ref(classification) and \\@ref(classification2).\n",
    "4. **Regression:**  \\index{regression!overview} predicting a quantitative value for a new observation. \n",
    "Regression is also used to answer predictive questions.\n",
    "For example, you might use regression to answer the following question:\n",
    "*What will be the race time for a 20-year-old runner who weighs 50kg?*\n",
    "Regression is covered in Chapters \\@ref(regression1) and \\@ref(regression2).\n",
    "5. **Clustering:** \\index{clustering!overview} finding previously unknown/unlabeled subgroups in a\n",
    "data set. Clustering is often used to answer exploratory questions.\n",
    "For example, you might use clustering to answer the following question:\n",
    "*What products are commonly bought together on Amazon?*\n",
    "Clustering is covered in Chapter \\@ref(clustering).\n",
    "6. **Estimation:**  \\index{estimation!overview} taking measurements for a small number of items from a large group \n",
    " and making a good guess for the average or proportion for the large group. Estimation \n",
    "is used to answer inferential questions.\n",
    "For example, you might use estimation to answer the following question:\n",
    "*Given a survey of cellphone ownership of 100 Canadians, what proportion\n",
    "of the entire Canadian population own Android phones?* \n",
    "Estimation is covered in Chapter \\@ref(inference).\n",
    "\n",
    "Referring to {numref}`questions-table`, our question about \n",
    "Aboriginal languages is an example of a *descriptive question*: we are\n",
    "summarizing the characteristics of a data set without further interpretation.\n",
    "And referring to the list above, it looks like we should use visualization\n",
    "and perhaps some summarization to answer the question. So in the remainder\n",
    "of this chapter, we will work towards making a visualization that shows \n",
    "us the ten most common Aboriginal languages in Canada and their associated counts,\n",
    "according to the 2016 census. \n",
    "\n",
    "## Loading a tabular data set\n",
    "A data set is, at its core essence, a structured collection of numbers and characters.\n",
    "Aside from that, there are really no strict rules; data sets can come in \n",
    "many different forms! Perhaps the most common form of data set that you will\n",
    "find in the wild, however, is *tabular data*\\index{tabular data}. Think spreadsheets in Microsoft Excel: tabular data are\n",
    "rectangular-shaped and spreadsheet-like, as shown in {numref}`img-spreadsheet-vs-dataframe`. In this book, we will focus primarily on tabular data.\n",
    "\n",
    "Since we are using Python for data analysis in this book, the first step for us is to\n",
    "load the data into Python. When we load tabular data into\n",
    "Python, it is represented as a *data frame* object\\index{data frame!overview}. {numref}`img-spreadsheet-vs-dataframe` shows that an Python data frame is very similar\n",
    "to a spreadsheet. We refer to the rows as \\index{observation} **observations**; these are the things that we\n",
    "collect the data on, e.g., voters, cities, etc. We refer to the columns as \\index{variable}\n",
    "**variables**; these are the characteristics of those observations, e.g., voters' political\n",
    "affiliations, cities' populations, etc. \n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/spreadsheet_vs_df.png\n",
    "---\n",
    "height: 400px\n",
    "name: img-spreadsheet-vs-dataframe\n",
    "---\n",
    "A spreadsheet versus a data frame in Python\n",
    "```\n",
    "\n",
    "The first kind of data file that we will learn how to load into Python as a data\n",
    "frame is the *comma-separated values* format (`.csv` for short)\\index{comma-separated values|see{csv}}\\index{csv}.  These files\n",
    "have names ending in `.csv`, and can be opened and saved using common\n",
    "spreadsheet programs like Microsoft Excel and Google Sheets.  For example, the\n",
    "`.csv` file named `can_lang.csv` \n",
    "is included with [the code for this book](https://github.com/UBC-DSCI/introduction-to-datascience/tree/master/data).\n",
    "If we were to open this data in a plain text editor (a program like Notepad that just shows\n",
    "text with no formatting), we would see each row on its own line, and each entry in the table separated by a comma:\n",
    "\n",
    "```code\n",
    "category,language,mother_tongue,most_at_home,most_at_work,lang_known\n",
    "Aboriginal languages,\"Aboriginal languages, n.o.s.\",590,235,30,665\n",
    "Non-Official & Non-Aboriginal languages,Afrikaans,10260,4785,85,23415\n",
    "Non-Official & Non-Aboriginal languages,\"Afro-Asiatic languages, n.i.e.\",1150,44\n",
    "Non-Official & Non-Aboriginal languages,Akan (Twi),13460,5985,25,22150\n",
    "Non-Official & Non-Aboriginal languages,Albanian,26895,13135,345,31930\n",
    "Aboriginal languages,\"Algonquian languages, n.i.e.\",45,10,0,120\n",
    "Aboriginal languages,Algonquin,1260,370,40,2480\n",
    "Non-Official & Non-Aboriginal languages,American Sign Language,2685,3020,1145,21\n",
    "Non-Official & Non-Aboriginal languages,Amharic,22465,12785,200,33670\n",
    "```\n",
    "\n",
    "To load this data into Python so that we can do things with it (e.g., perform\n",
    "analyses or create data visualizations), we will need to use a *function.* \\index{function} A\n",
    "function is a special word in Python that takes instructions (we call these\n",
    "*arguments*) \\index{argument} and does something. The function we will use to load a `.csv` file\n",
    "into Python is called `read_csv`. \\index{read function!read\\_csv} In its most basic \n",
    "use-case, `read_csv` expects that the data file:\n",
    "\n",
    "- has column names (or *headers*),\n",
    "- uses a comma (`,`) to separate the columns, and\n",
    "- does not have row names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b6490",
   "metadata": {},
   "source": [
    "Below you'll see the code used to load the data into Python using the `read_csv`\n",
    "function. Note that the `read_csv` function is not included in the base\n",
    "installation of Python, meaning that it is not one of the primary functions ready to\n",
    "use when you install Python. Therefore, you need to load it from somewhere else\n",
    "before you can use it. The place from which we will load it is called a Python *package*. \n",
    "A Python package \\index{package} is a collection of functions that can be used in addition to the\n",
    "built-in Python package functions once loaded. The `read_csv` function, in\n",
    "particular, can be made accessible by loading \n",
    "[the `pandas` Python package](https://pypi.org/project/pandas/) [@tidyverse; @wickham2019tidverse]\n",
    "using the `import` command. \\index{library} The `pandas` \\index{tidyverse} package contains many\n",
    "functions that we will use throughout this book to load, clean, wrangle, \n",
    "and visualize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c7bbea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.305948Z",
     "iopub.status.busy": "2022-06-08T02:12:12.305743Z",
     "iopub.status.idle": "2022-06-08T02:12:12.517726Z",
     "shell.execute_reply": "2022-06-08T02:12:12.517343Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658eda9c",
   "metadata": {},
   "source": [
    "After loading the `pandas` package and accessing it using the alias `pd`, we can call the `read_csv` function and\n",
    "pass it a single argument: the name of the file, `\"can_lang.csv\"`. We have to\n",
    "put quotes around file names and other letters and words that we use in our\n",
    "code to distinguish it from the special words (like functions!) that make up the Python programming\n",
    "language.  The file's name is the only argument we need to provide because our\n",
    "file satisfies everything else that the `read_csv` function expects in the default\n",
    "use-case. {numref}`img-read-csv` describes how we use the `read_csv`\n",
    "to read data into Python. \n",
    "\n",
    "(ref:img-read-csv) Syntax for the `read_csv` function.\n",
    "\n",
    "\n",
    "\n",
    "```{figure} img/read_csv_function.jpeg\n",
    "---\n",
    "height: 200px\n",
    "name: img-read-csv\n",
    "---\n",
    "Syntax for the read_csv function\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5218515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.519986Z",
     "iopub.status.busy": "2022-06-08T02:12:12.519753Z",
     "iopub.status.idle": "2022-06-08T02:12:12.532314Z",
     "shell.execute_reply": "2022-06-08T02:12:12.531949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>mother_tongue</th>\n",
       "      <th>most_at_home</th>\n",
       "      <th>most_at_work</th>\n",
       "      <th>lang_known</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Aboriginal languages, n.o.s.</td>\n",
       "      <td>590</td>\n",
       "      <td>235</td>\n",
       "      <td>30</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>10260</td>\n",
       "      <td>4785</td>\n",
       "      <td>85</td>\n",
       "      <td>23415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Afro-Asiatic languages, n.i.e.</td>\n",
       "      <td>1150</td>\n",
       "      <td>445</td>\n",
       "      <td>10</td>\n",
       "      <td>2775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Akan (Twi)</td>\n",
       "      <td>13460</td>\n",
       "      <td>5985</td>\n",
       "      <td>25</td>\n",
       "      <td>22150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Albanian</td>\n",
       "      <td>26895</td>\n",
       "      <td>13135</td>\n",
       "      <td>345</td>\n",
       "      <td>31930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Wolof</td>\n",
       "      <td>3990</td>\n",
       "      <td>1385</td>\n",
       "      <td>10</td>\n",
       "      <td>8240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Woods Cree</td>\n",
       "      <td>1840</td>\n",
       "      <td>800</td>\n",
       "      <td>75</td>\n",
       "      <td>2665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Wu (Shanghainese)</td>\n",
       "      <td>12915</td>\n",
       "      <td>7650</td>\n",
       "      <td>105</td>\n",
       "      <td>16530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Yiddish</td>\n",
       "      <td>13555</td>\n",
       "      <td>7085</td>\n",
       "      <td>895</td>\n",
       "      <td>20985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Yoruba</td>\n",
       "      <td>9080</td>\n",
       "      <td>2615</td>\n",
       "      <td>15</td>\n",
       "      <td>22415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    category                        language  \\\n",
       "0                       Aboriginal languages    Aboriginal languages, n.o.s.   \n",
       "1    Non-Official & Non-Aboriginal languages                       Afrikaans   \n",
       "2    Non-Official & Non-Aboriginal languages  Afro-Asiatic languages, n.i.e.   \n",
       "3    Non-Official & Non-Aboriginal languages                      Akan (Twi)   \n",
       "4    Non-Official & Non-Aboriginal languages                        Albanian   \n",
       "..                                       ...                             ...   \n",
       "209  Non-Official & Non-Aboriginal languages                           Wolof   \n",
       "210                     Aboriginal languages                      Woods Cree   \n",
       "211  Non-Official & Non-Aboriginal languages               Wu (Shanghainese)   \n",
       "212  Non-Official & Non-Aboriginal languages                         Yiddish   \n",
       "213  Non-Official & Non-Aboriginal languages                          Yoruba   \n",
       "\n",
       "     mother_tongue  most_at_home  most_at_work  lang_known  \n",
       "0              590           235            30         665  \n",
       "1            10260          4785            85       23415  \n",
       "2             1150           445            10        2775  \n",
       "3            13460          5985            25       22150  \n",
       "4            26895         13135           345       31930  \n",
       "..             ...           ...           ...         ...  \n",
       "209           3990          1385            10        8240  \n",
       "210           1840           800            75        2665  \n",
       "211          12915          7650           105       16530  \n",
       "212          13555          7085           895       20985  \n",
       "213           9080          2615            15       22415  \n",
       "\n",
       "[214 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/can_lang.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d8097",
   "metadata": {},
   "source": [
    "## Naming things in Python\n",
    "\n",
    "When we loaded the 2016 Canadian census language data\n",
    "using `read_csv`, we did not give this data frame a name. \n",
    "Therefore the data was just printed on the screen, \n",
    "and we cannot do anything else with it. That isn't very useful. \n",
    "What would be more useful would be to give a name \n",
    "to the data frame that `read_csv` outputs, \n",
    "so that we can refer to it later for analysis and visualization.\n",
    "\n",
    "The way to assign a name to a value in Python is via the *assignment symbol* `=`. \n",
    "\\index{aaaassignsymb@\\texttt{<-}|see{assignment symbol}}\\index{assignment symbol}\n",
    "On the left side of the assignment symbol you put the name that you want\n",
    "to use, and on the right side of the assignment symbol\n",
    "you put the value that you want the name to refer to.\n",
    "Names can be used to refer to almost anything in Python, such as numbers,\n",
    "words (also known as *strings* of characters), and data frames!\n",
    "Below, we set `my_number` to `3` (the result of `1+2`)\n",
    "and we set `name` to the string `\"Alice\"`. \\index{string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c3d8ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.534357Z",
     "iopub.status.busy": "2022-06-08T02:12:12.534209Z",
     "iopub.status.idle": "2022-06-08T02:12:12.536645Z",
     "shell.execute_reply": "2022-06-08T02:12:12.536327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Alice\n"
     ]
    }
   ],
   "source": [
    "my_number = 1 + 2\n",
    "print(my_number)\n",
    "\n",
    "name = \"Alice\"\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843b2c3",
   "metadata": {},
   "source": [
    "Note that when \n",
    "we name something in Python using the assignment symbol, `=`, \n",
    "we do not need to surround the name we are creating  with quotes. This is \n",
    "because we are formally telling Python that this special word denotes\n",
    "the value of whatever is on the right-hand side.\n",
    "Only characters and words that act as *values* on the right-hand side of the assignment\n",
    "symbol&mdash;e.g., the file name `\"data/can_lang.csv\"` that we specified before, or `\"Alice\"` above&mdash;need \n",
    "to be surrounded by quotes.\n",
    "\n",
    "After making the assignment, we can use the special name words we have created in\n",
    "place of their values. For example, if we want to do something with the value `3` later on, \n",
    "we can just use `my_number` instead. Let's try adding 2 to `my_number`; you will see that\n",
    "Python just interprets this as adding 2 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1d5cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.538528Z",
     "iopub.status.busy": "2022-06-08T02:12:12.538385Z",
     "iopub.status.idle": "2022-06-08T02:12:12.540853Z",
     "shell.execute_reply": "2022-06-08T02:12:12.540520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_number + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47be1c2",
   "metadata": {},
   "source": [
    "Object names \\index{object} can consist of letters, numbers, periods `.` and underscores `_`.\n",
    "Other symbols won't work since they have their own meanings in Python. For example,\n",
    "`+` is the addition symbol(operator); if we try to assign a name with\n",
    "the `+` symbol, Python will complain and we will get an error!\n",
    "\n",
    "\n",
    "```\n",
    "na+me = 1\n",
    "```\n",
    "\n",
    "```\n",
    "SyntaxError: cannot assign to operator\n",
    "```\n",
    "\n",
    "There are certain conventions for naming objects in Python. \n",
    "When naming \\index{object!naming convention} an object we\n",
    "suggest using only lowercase letters, numbers and underscores `_` to separate\n",
    "the words in a name.  Python is case sensitive, which means that `Letter` and\n",
    "`letter` would be two different objects in Python.  You should also try to give your\n",
    "objects meaningful names.  For instance, you *can* name a data frame `x`.\n",
    "However, using more meaningful terms, such as `language_data`, will help you\n",
    "remember what each name in your code represents.  We recommend following the\n",
    "**PEP 8** naming conventions outlined in the *[PEP 8](https://peps.python.org/pep-0008/)* [@tidyversestyleguide].  Let's\n",
    "now use the assignment symbol to give the name\n",
    "`can_lang` to the 2016 Canadian census language data frame that we get from\n",
    "`read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c2a352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.542667Z",
     "iopub.status.busy": "2022-06-08T02:12:12.542525Z",
     "iopub.status.idle": "2022-06-08T02:12:12.545916Z",
     "shell.execute_reply": "2022-06-08T02:12:12.545491Z"
    }
   },
   "outputs": [],
   "source": [
    "can_lang = pd.read_csv(\"data/can_lang.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e58e4",
   "metadata": {},
   "source": [
    "Wait a minute, nothing happened this time! Where's our data?\n",
    "Actually, something did happen: the data was loaded in \n",
    "and now has the name `can_lang` associated with it. \n",
    "And we can use that name to access the data frame and do things with it. \n",
    "For example, we can type the name of the data frame to print the first few rows \n",
    "on the screen. You will also see at the top that the number of observations (i.e., rows) and \n",
    "variables (i.e., columns) are printed. Printing the first few rows of a data frame \n",
    "like this is a handy way to get a quick sense for what is contained in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "713e6830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.547729Z",
     "iopub.status.busy": "2022-06-08T02:12:12.547604Z",
     "iopub.status.idle": "2022-06-08T02:12:12.553457Z",
     "shell.execute_reply": "2022-06-08T02:12:12.553135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>mother_tongue</th>\n",
       "      <th>most_at_home</th>\n",
       "      <th>most_at_work</th>\n",
       "      <th>lang_known</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Aboriginal languages, n.o.s.</td>\n",
       "      <td>590</td>\n",
       "      <td>235</td>\n",
       "      <td>30</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>10260</td>\n",
       "      <td>4785</td>\n",
       "      <td>85</td>\n",
       "      <td>23415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Afro-Asiatic languages, n.i.e.</td>\n",
       "      <td>1150</td>\n",
       "      <td>445</td>\n",
       "      <td>10</td>\n",
       "      <td>2775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Akan (Twi)</td>\n",
       "      <td>13460</td>\n",
       "      <td>5985</td>\n",
       "      <td>25</td>\n",
       "      <td>22150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Albanian</td>\n",
       "      <td>26895</td>\n",
       "      <td>13135</td>\n",
       "      <td>345</td>\n",
       "      <td>31930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Wolof</td>\n",
       "      <td>3990</td>\n",
       "      <td>1385</td>\n",
       "      <td>10</td>\n",
       "      <td>8240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Woods Cree</td>\n",
       "      <td>1840</td>\n",
       "      <td>800</td>\n",
       "      <td>75</td>\n",
       "      <td>2665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Wu (Shanghainese)</td>\n",
       "      <td>12915</td>\n",
       "      <td>7650</td>\n",
       "      <td>105</td>\n",
       "      <td>16530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Yiddish</td>\n",
       "      <td>13555</td>\n",
       "      <td>7085</td>\n",
       "      <td>895</td>\n",
       "      <td>20985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Non-Official &amp; Non-Aboriginal languages</td>\n",
       "      <td>Yoruba</td>\n",
       "      <td>9080</td>\n",
       "      <td>2615</td>\n",
       "      <td>15</td>\n",
       "      <td>22415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    category                        language  \\\n",
       "0                       Aboriginal languages    Aboriginal languages, n.o.s.   \n",
       "1    Non-Official & Non-Aboriginal languages                       Afrikaans   \n",
       "2    Non-Official & Non-Aboriginal languages  Afro-Asiatic languages, n.i.e.   \n",
       "3    Non-Official & Non-Aboriginal languages                      Akan (Twi)   \n",
       "4    Non-Official & Non-Aboriginal languages                        Albanian   \n",
       "..                                       ...                             ...   \n",
       "209  Non-Official & Non-Aboriginal languages                           Wolof   \n",
       "210                     Aboriginal languages                      Woods Cree   \n",
       "211  Non-Official & Non-Aboriginal languages               Wu (Shanghainese)   \n",
       "212  Non-Official & Non-Aboriginal languages                         Yiddish   \n",
       "213  Non-Official & Non-Aboriginal languages                          Yoruba   \n",
       "\n",
       "     mother_tongue  most_at_home  most_at_work  lang_known  \n",
       "0              590           235            30         665  \n",
       "1            10260          4785            85       23415  \n",
       "2             1150           445            10        2775  \n",
       "3            13460          5985            25       22150  \n",
       "4            26895         13135           345       31930  \n",
       "..             ...           ...           ...         ...  \n",
       "209           3990          1385            10        8240  \n",
       "210           1840           800            75        2665  \n",
       "211          12915          7650           105       16530  \n",
       "212          13555          7085           895       20985  \n",
       "213           9080          2615            15       22415  \n",
       "\n",
       "[214 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c62c66",
   "metadata": {},
   "source": [
    "## Creating subsets of data frames with `df[]` & `loc[]`\n",
    "\n",
    "Now that we've loaded our data into Python, we can start wrangling the data to\n",
    "find the ten Aboriginal languages that were most often reported\n",
    "in 2016 as mother tongues in Canada. In particular, we will construct \n",
    "a table with the ten Aboriginal languages that have the largest \n",
    "counts in the `mother_tongue` column. \n",
    "The `df[]` and `loc[]` properties of the `pandas` dataframe will help us\n",
    "here. The `df[]` \\index{filter} property allows you to obtain a subset of the\n",
    "rows with specific values, while the `loc[]` \\index{select} property allows you \n",
    "to obtain a subset of the columns. Therefore, we can use `df[]` \n",
    "to filter the rows to extract the Aboriginal languages in the data set, and \n",
    "then use `loc[]` property to obtain only the columns we want to include in our table.\n",
    "\n",
    "### Using `df[]` to extract rows\n",
    "Looking at the `can_lang` data above, we see the column `category` contains different\n",
    "high-level categories of languages, which include \"Aboriginal languages\",\n",
    "\"Non-Official & Non-Aboriginal languages\" and \"Official languages\".  To answer\n",
    "our question we want to filter our data set so we restrict our attention \n",
    "to only those languages in the \"Aboriginal languages\" category. \n",
    "\n",
    "We can use the `df[]` \\index{filter} property to obtain the subset of rows with desired\n",
    "values from a data frame, where `df[]` is same as the data frame object you are using. \n",
    "In our case it would be `can_lang[]`\n",
    "The argument to `can_lang[]` is selection of column name `category` of the data frame\n",
    "object, `can_lang`. The second argument is a *logical statement* \\index{logical statement} to use when\n",
    "filtering the rows with the logical statement evaluating to either `TRUE` or `FALSE`;\n",
    "`df[]` keeps only those rows for which the logical statement evaluates to `TRUE`.\n",
    "For example, in our analysis, we are interested in keeping only languages in the\n",
    "\"Aboriginal languages\" higher-level category. We can use \n",
    "the *equivalency operator* `==` \\index{logical statement!equivalency operator} to compare the values\n",
    "of the `category` column with the value `\"Aboriginal languages\"`; you will learn about\n",
    "many other kinds of logical statements in Chapter \\@ref(wrangling).  Similar to\n",
    "when we loaded the data file and put quotes around the file name, here we need\n",
    "to put quotes around `\"Aboriginal languages\"`. Using quotes tells Python that this\n",
    "is a string *value* \\index{string} and not one of the special words that make up Python\n",
    "programming language, or one of the names we have given to data frames in the\n",
    "code we have already written. \n",
    "\n",
    "\n",
    "\n",
    "With the logical filter statement inside `df[]`, we get a data frame that has all the columns of\n",
    "the input data frame, but only those rows we asked for in our logical filter\n",
    "statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca3fd05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.555422Z",
     "iopub.status.busy": "2022-06-08T02:12:12.555273Z",
     "iopub.status.idle": "2022-06-08T02:12:12.561806Z",
     "shell.execute_reply": "2022-06-08T02:12:12.561457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>mother_tongue</th>\n",
       "      <th>most_at_home</th>\n",
       "      <th>most_at_work</th>\n",
       "      <th>lang_known</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Aboriginal languages, n.o.s.</td>\n",
       "      <td>590</td>\n",
       "      <td>235</td>\n",
       "      <td>30</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Algonquian languages, n.i.e.</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Algonquin</td>\n",
       "      <td>1260</td>\n",
       "      <td>370</td>\n",
       "      <td>40</td>\n",
       "      <td>2480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Athabaskan languages, n.i.e.</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Atikamekw</td>\n",
       "      <td>6150</td>\n",
       "      <td>5465</td>\n",
       "      <td>1100</td>\n",
       "      <td>6645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Thompson (Ntlakapamux)</td>\n",
       "      <td>335</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Tlingit</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Tsimshian</td>\n",
       "      <td>200</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Wakashan languages, n.i.e.</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Aboriginal languages</td>\n",
       "      <td>Woods Cree</td>\n",
       "      <td>1840</td>\n",
       "      <td>800</td>\n",
       "      <td>75</td>\n",
       "      <td>2665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 category                      language  mother_tongue  \\\n",
       "0    Aboriginal languages  Aboriginal languages, n.o.s.            590   \n",
       "5    Aboriginal languages  Algonquian languages, n.i.e.             45   \n",
       "6    Aboriginal languages                     Algonquin           1260   \n",
       "12   Aboriginal languages  Athabaskan languages, n.i.e.             50   \n",
       "13   Aboriginal languages                     Atikamekw           6150   \n",
       "..                    ...                           ...            ...   \n",
       "191  Aboriginal languages        Thompson (Ntlakapamux)            335   \n",
       "195  Aboriginal languages                       Tlingit             95   \n",
       "196  Aboriginal languages                     Tsimshian            200   \n",
       "206  Aboriginal languages    Wakashan languages, n.i.e.             10   \n",
       "210  Aboriginal languages                    Woods Cree           1840   \n",
       "\n",
       "     most_at_home  most_at_work  lang_known  \n",
       "0             235            30         665  \n",
       "5              10             0         120  \n",
       "6             370            40        2480  \n",
       "12             10             0          85  \n",
       "13           5465          1100        6645  \n",
       "..            ...           ...         ...  \n",
       "191            20             0         450  \n",
       "195             0            10         260  \n",
       "196            30            10         410  \n",
       "206             0             0          25  \n",
       "210           800            75        2665  \n",
       "\n",
       "[67 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aboriginal_lang = can_lang[can_lang['category'] == 'Aboriginal languages']\n",
    "aboriginal_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7a0a4",
   "metadata": {},
   "source": [
    "It's good practice to check the output after using a\n",
    "function in Python. We can see the original `can_lang` data set contained 214 rows\n",
    "with multiple kinds of `category`. The data frame\n",
    "`aboriginal_lang` contains only 67 rows, and looks like it only contains languages in\n",
    "the \"Aboriginal languages\" in the `category` column. So it looks like the `df[]` property\n",
    "gave us the result we wanted!\n",
    "\n",
    "### Using `.loc[]` to extract columns\n",
    "\n",
    "Now let's use `loc[]` \\index{select} to extract the `language` and `mother_tongue` columns\n",
    "from this data frame. To extract these columns, we need to provide the `loc[]`\n",
    "property with list of rows separated by list of columns i.e `df.loc[[row1, row2, ...], [col1, col2, ...]]`\n",
    "As we want to access all the rows of the dataframe, instead of passing the names of all the rows, \n",
    "we can instead just depict them using `:` and then pass a list of columns `language` and `mother_tongue`.\n",
    "After passing these as arguments, the  `loc[]` property\n",
    "returns two columns (the `language` and `mother_tongue` columns that we asked\n",
    "for) as a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece82b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.563673Z",
     "iopub.status.busy": "2022-06-08T02:12:12.563518Z",
     "iopub.status.idle": "2022-06-08T02:12:12.568905Z",
     "shell.execute_reply": "2022-06-08T02:12:12.568600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>mother_tongue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aboriginal languages, n.o.s.</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Algonquian languages, n.i.e.</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Algonquin</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Athabaskan languages, n.i.e.</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Atikamekw</td>\n",
       "      <td>6150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Thompson (Ntlakapamux)</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Tlingit</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Tsimshian</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Wakashan languages, n.i.e.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Woods Cree</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         language  mother_tongue\n",
       "0    Aboriginal languages, n.o.s.            590\n",
       "5    Algonquian languages, n.i.e.             45\n",
       "6                       Algonquin           1260\n",
       "12   Athabaskan languages, n.i.e.             50\n",
       "13                      Atikamekw           6150\n",
       "..                            ...            ...\n",
       "191        Thompson (Ntlakapamux)            335\n",
       "195                       Tlingit             95\n",
       "196                     Tsimshian            200\n",
       "206    Wakashan languages, n.i.e.             10\n",
       "210                    Woods Cree           1840\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_lang = aboriginal_lang.loc[:, ['language', 'mother_tongue']]\n",
    "selected_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd7111",
   "metadata": {},
   "source": [
    "### Using `sort_values` to order and `iloc[]` to select rows by index number\n",
    "\n",
    "We have used `df[]` and `loc[]` properties of dataframe to obtain a table with only the Aboriginal\n",
    "languages in the data set and their associated counts. However, we want to know\n",
    "the **ten** languages that are spoken most often. As a next step, we could\n",
    "order the `mother_tongue` column from greatest to least and then extract only\n",
    "the top ten rows. This is where the `sort_values` function and `.iloc[]` property come to the\n",
    "rescue! \\index{arrange}\\index{slice}\n",
    "\n",
    "The `sort_values` function allows us to order the rows of a data frame by the\n",
    "values of a particular column.  We need to specify the column name\n",
    "by which we want to sort the dataframe by passing it to the argument `by`.\n",
    "Since we want to choose the ten Aboriginal languages most often reported as a mother tongue\n",
    "language, we will use the `sort_values` function to order the rows in our\n",
    "`selected_lang` data frame by the `mother_tongue` column. We want to\n",
    "arrange the rows in descending order (from largest to smallest),\n",
    "so we specify the argument `ascending` as `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e664e2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.570699Z",
     "iopub.status.busy": "2022-06-08T02:12:12.570557Z",
     "iopub.status.idle": "2022-06-08T02:12:12.575276Z",
     "shell.execute_reply": "2022-06-08T02:12:12.575001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>mother_tongue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Cree, n.o.s.</td>\n",
       "      <td>64050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Inuktitut</td>\n",
       "      <td>35210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Ojibway</td>\n",
       "      <td>17885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Oji-Cree</td>\n",
       "      <td>12855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Dene</td>\n",
       "      <td>10700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Algonquian languages, n.i.e.</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Cayuga</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Squamish</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Iroquoian languages, n.i.e.</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Wakashan languages, n.i.e.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         language  mother_tongue\n",
       "40                   Cree, n.o.s.          64050\n",
       "89                      Inuktitut          35210\n",
       "138                       Ojibway          17885\n",
       "137                      Oji-Cree          12855\n",
       "48                           Dene          10700\n",
       "..                            ...            ...\n",
       "5    Algonquian languages, n.i.e.             45\n",
       "32                         Cayuga             45\n",
       "179                      Squamish             40\n",
       "90    Iroquoian languages, n.i.e.             35\n",
       "206    Wakashan languages, n.i.e.             10\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arranged_lang = selected_lang.sort_values(by='mother_tongue', ascending=False)\n",
    "arranged_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd1202",
   "metadata": {},
   "source": [
    "Next we will use the `iloc[]` property, which selects rows according to their\n",
    "row number. Since we want to choose the most common ten languages, we will indicate we want the\n",
    "rows 1 to 10 using the argument `:10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3298d4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.576989Z",
     "iopub.status.busy": "2022-06-08T02:12:12.576853Z",
     "iopub.status.idle": "2022-06-08T02:12:12.580564Z",
     "shell.execute_reply": "2022-06-08T02:12:12.580255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>mother_tongue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Cree, n.o.s.</td>\n",
       "      <td>64050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Inuktitut</td>\n",
       "      <td>35210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Ojibway</td>\n",
       "      <td>17885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Oji-Cree</td>\n",
       "      <td>12855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Dene</td>\n",
       "      <td>10700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Montagnais (Innu)</td>\n",
       "      <td>10235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Mi'kmaq</td>\n",
       "      <td>6690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Atikamekw</td>\n",
       "      <td>6150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Plains Cree</td>\n",
       "      <td>3065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Stoney</td>\n",
       "      <td>3025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              language  mother_tongue\n",
       "40        Cree, n.o.s.          64050\n",
       "89           Inuktitut          35210\n",
       "138            Ojibway          17885\n",
       "137           Oji-Cree          12855\n",
       "48                Dene          10700\n",
       "125  Montagnais (Innu)          10235\n",
       "119            Mi'kmaq           6690\n",
       "13           Atikamekw           6150\n",
       "149        Plains Cree           3065\n",
       "180             Stoney           3025"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_lang = arranged_lang.iloc[:10]\n",
    "ten_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3ff48",
   "metadata": {},
   "source": [
    "We have now answered our initial question by generating this table!\n",
    "Are we done? Well, not quite; tables are almost never the best way to present\n",
    "the result of your analysis to your audience. Even the simple table above with\n",
    "only two columns presents some difficulty: for example, you have to scrutinize\n",
    "the table quite closely to get a sense for the relative numbers of speakers of \n",
    "each language. When you move on to more complicated analyses, this issue only \n",
    "gets worse. In contrast, a *visualization* would convey this information in a much \n",
    "more easily understood format. \n",
    "Visualizations are a great tool for summarizing information to help you\n",
    "effectively communicate with your audience. \n",
    "\n",
    "## Exploring data with visualizations\n",
    "Creating effective data visualizations \\index{visualization} is an essential component of any data\n",
    "analysis. In this section we will develop a visualization of the \n",
    " ten Aboriginal languages that were most often reported in 2016 as mother tongues in\n",
    "Canada, as well as the number of people that speak each of them.\n",
    "\n",
    "### Using `altair` to create a bar plot\n",
    "\n",
    "In our data set, we can see that `language` and `mother_tongue` are in separate\n",
    "columns (or variables). In addition, there is a single row (or observation) for each language.\n",
    "The data are, therefore, in what we call a *tidy data* format. Tidy data is a\n",
    "fundamental concept and will be a significant focus in the remainder of this\n",
    "book: many of the functions from `pandas` require tidy data, including the\n",
    "`altair` \\index{ggplot} package that we will use shortly for our visualization. We will\n",
    "formally introduce tidy data in Chapter \\@ref(wrangling).\n",
    "\n",
    "We will make a bar plot to visualize our data. A bar plot \\index{plot|see{visualization}}\\index{visualization|see{ggplot}}\\index{visualization!bar} is a chart where the\n",
    "heights of the bars represent certain values, like counts or proportions. We\n",
    "will make a bar plot using the `mother_tongue` and `language` columns from our\n",
    "`ten_lang` data frame. To create a bar plot of these two variables using the \n",
    "`altair` package, we must specify the data frame, which variables\n",
    "to put on the x and y axes, and what kind of plot to create. \n",
    "First, we need to import the `altair` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1646b1f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.582277Z",
     "iopub.status.busy": "2022-06-08T02:12:12.582160Z",
     "iopub.status.idle": "2022-06-08T02:12:12.669179Z",
     "shell.execute_reply": "2022-06-08T02:12:12.668793Z"
    }
   },
   "outputs": [],
   "source": [
    "import altair as alt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba33898",
   "metadata": {},
   "source": [
    "The fundamental object in Altair is the `Chart`, which takes a data frame as a single argument `alt.Chart(ten_lang)`.\n",
    "With a chart object in hand, we can now specify how we would like the data to be visualized. \n",
    "We first indicate what kind of geometric mark we want to use to represent the data. We can set the mark attribute \n",
    "of the chart object using the `Chart.mark_*` methods.\n",
    "Here, as we want to plot the bar chart, so we will use `mark_bar()` method.\n",
    "Next, we need to encode the fields of the data frame using \n",
    "the `x`(represents the x-axis position of the points) and \n",
    "`y`(represents the y-axis position of the points) channels. The `encode()`\n",
    "method builds a key-value mapping between encoding channels (such as x, y) \n",
    "to fields in the dataset, accessed by field name(column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581967aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.671306Z",
     "iopub.status.busy": "2022-06-08T02:12:12.671158Z",
     "iopub.status.idle": "2022-06-08T02:12:12.678061Z",
     "shell.execute_reply": "2022-06-08T02:12:12.677759Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "barplot_mother_tongue = (\n",
    "    alt.Chart(ten_lang)\n",
    "    .mark_bar().encode(\n",
    "        x='language',\n",
    "        y='mother_tongue'\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a018b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.679838Z",
     "iopub.status.busy": "2022-06-08T02:12:12.679697Z",
     "iopub.status.idle": "2022-06-08T02:12:12.689614Z",
     "shell.execute_reply": "2022-06-08T02:12:12.689307Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-50dcec261c1f4453bc879bb8516aad46\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-50dcec261c1f4453bc879bb8516aad46\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-50dcec261c1f4453bc879bb8516aad46\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0cdfcf8245507d928490a47b75d8fff0\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"language\", \"type\": \"nominal\"}, \"y\": {\"field\": \"mother_tongue\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-0cdfcf8245507d928490a47b75d8fff0\": [{\"language\": \"Cree, n.o.s.\", \"mother_tongue\": 64050}, {\"language\": \"Inuktitut\", \"mother_tongue\": 35210}, {\"language\": \"Ojibway\", \"mother_tongue\": 17885}, {\"language\": \"Oji-Cree\", \"mother_tongue\": 12855}, {\"language\": \"Dene\", \"mother_tongue\": 10700}, {\"language\": \"Montagnais (Innu)\", \"mother_tongue\": 10235}, {\"language\": \"Mi'kmaq\", \"mother_tongue\": 6690}, {\"language\": \"Atikamekw\", \"mother_tongue\": 6150}, {\"language\": \"Plains Cree\", \"mother_tongue\": 3065}, {\"language\": \"Stoney\", \"mother_tongue\": 3025}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "barplot-mother-tongue"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue('barplot-mother-tongue', barplot_mother_tongue, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6439e",
   "metadata": {},
   "source": [
    ":::{glue:figure} barplot-mother-tongue\n",
    ":figwidth: 700px\n",
    ":name: barplot-mother-tongue\n",
    "\n",
    "Bar plot of the ten Aboriginal languages most often reported by Canadian residents as their mother tongue\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975e19c",
   "metadata": {},
   "source": [
    "> **Note:** The vast majority of the\n",
    "> time, a single expression in Python must be contained in a single line of code.\n",
    "> However, there *are* a small number of situations in which you can have a\n",
    "> single Python expression span multiple lines. Above is one such case: here, Python knows that a line cannot\n",
    "> end with a `.` symbol, \\index{aaaplussymb@$+$|see{ggplot (add layer)}} and so it keeps reading the next line to figure out\n",
    "> what the right-hand side of the `.` symbol should be.  We could, of course,\n",
    "> put all of the added layers on one line of code, but splitting them across\n",
    "> multiple lines helps a lot with code readability. \\index{multi-line expression}\n",
    "\n",
    "### Formatting altair objects\n",
    "\n",
    "It is exciting that we can already visualize our data to help answer our\n",
    "question, but we are not done yet! We can (and should) do more to improve the\n",
    "interpretability of the data visualization that we created. For example, by\n",
    "default, Python uses the column names as the axis labels. Usually these\n",
    "column names do not have enough information about the variable in the column.\n",
    "We really should replace this default with a more informative label. For the\n",
    "example above, Python uses the column name `mother_tongue` as the label for the\n",
    "y axis, but most people will not know what that is. And even if they did, they\n",
    "will not know how we measured this variable, or the group of people on which the\n",
    "measurements were taken. An axis label that reads \"Mother Tongue (Number of\n",
    "Canadian Residents)\" would be much more informative.\n",
    "\n",
    "Adding additional labels \\index{plot!layers} to our visualizations that we create in `altair` is\n",
    "one common and easy way to improve and refine our data visualizations. We can add titles for the axes \n",
    "in the `altair` objects using `alt.X` and `alt.Y` with the `title` argument to make \n",
    "the axes titles more informative.\n",
    "\\index{plot!axis labels} Again, since we are specifying\n",
    "words (e.g. `\"Mother Tongue (Number of Canadian Residents)\"`) as arguments to\n",
    "`alt.X` and `alt.Y`, we surround them with double quotation marks. We can do many other modifications\n",
    "to format the plot further, and we will explore these in Chapter\n",
    "\\@ref(viz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c05658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.691447Z",
     "iopub.status.busy": "2022-06-08T02:12:12.691304Z",
     "iopub.status.idle": "2022-06-08T02:12:12.693705Z",
     "shell.execute_reply": "2022-06-08T02:12:12.693392Z"
    }
   },
   "outputs": [],
   "source": [
    "barplot_mother_tongue = (\n",
    "    alt.Chart(ten_lang)\n",
    "    .mark_bar().encode(\n",
    "        x=alt.X('language', title='Language'),\n",
    "        y=alt.Y('mother_tongue', title='Mother Tongue (Number of Canadian Residents)')\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd515db5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.695402Z",
     "iopub.status.busy": "2022-06-08T02:12:12.695266Z",
     "iopub.status.idle": "2022-06-08T02:12:12.699943Z",
     "shell.execute_reply": "2022-06-08T02:12:12.699658Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-77b1fecffdcf4cc4a398ca0838777587\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-77b1fecffdcf4cc4a398ca0838777587\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-77b1fecffdcf4cc4a398ca0838777587\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0cdfcf8245507d928490a47b75d8fff0\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"language\", \"title\": \"Language\", \"type\": \"nominal\"}, \"y\": {\"field\": \"mother_tongue\", \"title\": \"Mother Tongue (Number of Canadian Residents)\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-0cdfcf8245507d928490a47b75d8fff0\": [{\"language\": \"Cree, n.o.s.\", \"mother_tongue\": 64050}, {\"language\": \"Inuktitut\", \"mother_tongue\": 35210}, {\"language\": \"Ojibway\", \"mother_tongue\": 17885}, {\"language\": \"Oji-Cree\", \"mother_tongue\": 12855}, {\"language\": \"Dene\", \"mother_tongue\": 10700}, {\"language\": \"Montagnais (Innu)\", \"mother_tongue\": 10235}, {\"language\": \"Mi'kmaq\", \"mother_tongue\": 6690}, {\"language\": \"Atikamekw\", \"mother_tongue\": 6150}, {\"language\": \"Plains Cree\", \"mother_tongue\": 3065}, {\"language\": \"Stoney\", \"mother_tongue\": 3025}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "barplot-mother-tongue-labs"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue('barplot-mother-tongue-labs', barplot_mother_tongue, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d8728",
   "metadata": {},
   "source": [
    ":::{glue:figure} barplot-mother-tongue-labs\n",
    ":figwidth: 700px\n",
    ":name: barplot-mother-tongue-labs\n",
    "\n",
    "Bar plot of the ten Aboriginal languages most often reported by Canadian residents as their mother tongue with x and y labels. Note that this visualization is not done yet; there are still improvements to be made.\n",
    ":::\n",
    "\n",
    "\n",
    "The result is shown in {numref}`barplot-mother-tongue-labs`. \n",
    "This is already quite an improvement! Let's tackle the next major issue with the visualization\n",
    "in {numref}`barplot-mother-tongue-labs`: the vertical x axis labels, which are\n",
    "currently making it difficult to read the different language names.\n",
    "One solution is to rotate the plot such that the bars are horizontal rather than vertical.\n",
    "To accomplish this, we will swap the x and y coordinate axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4e15450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.701814Z",
     "iopub.status.busy": "2022-06-08T02:12:12.701660Z",
     "iopub.status.idle": "2022-06-08T02:12:12.704049Z",
     "shell.execute_reply": "2022-06-08T02:12:12.703751Z"
    }
   },
   "outputs": [],
   "source": [
    "barplot_mother_tongue_axis = (\n",
    "    alt.Chart(ten_lang)\n",
    "    .mark_bar().encode(\n",
    "        x=alt.X('mother_tongue', title='Mother Tongue (Number of Canadian Residents)'),\n",
    "        y=alt.Y('language', title='Language')\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f5412d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.705889Z",
     "iopub.status.busy": "2022-06-08T02:12:12.705750Z",
     "iopub.status.idle": "2022-06-08T02:12:12.710506Z",
     "shell.execute_reply": "2022-06-08T02:12:12.710225Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-7dcc540eb1334736913a59d0b2083270\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7dcc540eb1334736913a59d0b2083270\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7dcc540eb1334736913a59d0b2083270\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0cdfcf8245507d928490a47b75d8fff0\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"mother_tongue\", \"title\": \"Mother Tongue (Number of Canadian Residents)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"language\", \"title\": \"Language\", \"type\": \"nominal\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-0cdfcf8245507d928490a47b75d8fff0\": [{\"language\": \"Cree, n.o.s.\", \"mother_tongue\": 64050}, {\"language\": \"Inuktitut\", \"mother_tongue\": 35210}, {\"language\": \"Ojibway\", \"mother_tongue\": 17885}, {\"language\": \"Oji-Cree\", \"mother_tongue\": 12855}, {\"language\": \"Dene\", \"mother_tongue\": 10700}, {\"language\": \"Montagnais (Innu)\", \"mother_tongue\": 10235}, {\"language\": \"Mi'kmaq\", \"mother_tongue\": 6690}, {\"language\": \"Atikamekw\", \"mother_tongue\": 6150}, {\"language\": \"Plains Cree\", \"mother_tongue\": 3065}, {\"language\": \"Stoney\", \"mother_tongue\": 3025}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "barplot-mother-tongue-labs-axis"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue('barplot-mother-tongue-labs-axis', barplot_mother_tongue_axis, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a70011",
   "metadata": {},
   "source": [
    ":::{glue:figure} barplot-mother-tongue-labs-axis\n",
    ":figwidth: 700px\n",
    ":name: barplot-mother-tongue-labs-axis\n",
    "\n",
    "Horizontal bar plot of the ten Aboriginal languages most often reported by Canadian residents as their mother tongue. There are no more serious issues with this visualization, but it could be refined further.\n",
    ":::\n",
    "\n",
    "Another big step forward, as shown in {numref}`barplot-mother-tongue-labs-axis`! There \n",
    "are no more serious issues with the visualization. Now comes time to refine\n",
    "the visualization to make it even more well-suited to answering the question\n",
    "we asked earlier in this chapter. For example, the visualization could be made more transparent by\n",
    "organizing the bars according to the number of Canadian residents reporting\n",
    "each language, rather than in alphabetical order. We can reorder the bars using\n",
    "the `sort` \\index{reorder} argument, which orders a variable (here `language`) based on the\n",
    "values of the variable(`mother_tongue`) on the `x-axis`. \n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19058158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.712277Z",
     "iopub.status.busy": "2022-06-08T02:12:12.712136Z",
     "iopub.status.idle": "2022-06-08T02:12:12.714631Z",
     "shell.execute_reply": "2022-06-08T02:12:12.714354Z"
    }
   },
   "outputs": [],
   "source": [
    "ordered_barplot_mother_tongue = (\n",
    "    alt.Chart(ten_lang)\n",
    "    .mark_bar().encode(\n",
    "        x=alt.X('mother_tongue', title='Mother Tongue (Number of Canadian Residents)'),\n",
    "        y=alt.Y('language', sort='x', title='Language')\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1032f49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.716368Z",
     "iopub.status.busy": "2022-06-08T02:12:12.716233Z",
     "iopub.status.idle": "2022-06-08T02:12:12.720905Z",
     "shell.execute_reply": "2022-06-08T02:12:12.720604Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-e68effe75a38436e867bc2c9bfed96a0\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e68effe75a38436e867bc2c9bfed96a0\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e68effe75a38436e867bc2c9bfed96a0\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0cdfcf8245507d928490a47b75d8fff0\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"mother_tongue\", \"title\": \"Mother Tongue (Number of Canadian Residents)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"language\", \"sort\": \"x\", \"title\": \"Language\", \"type\": \"nominal\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-0cdfcf8245507d928490a47b75d8fff0\": [{\"language\": \"Cree, n.o.s.\", \"mother_tongue\": 64050}, {\"language\": \"Inuktitut\", \"mother_tongue\": 35210}, {\"language\": \"Ojibway\", \"mother_tongue\": 17885}, {\"language\": \"Oji-Cree\", \"mother_tongue\": 12855}, {\"language\": \"Dene\", \"mother_tongue\": 10700}, {\"language\": \"Montagnais (Innu)\", \"mother_tongue\": 10235}, {\"language\": \"Mi'kmaq\", \"mother_tongue\": 6690}, {\"language\": \"Atikamekw\", \"mother_tongue\": 6150}, {\"language\": \"Plains Cree\", \"mother_tongue\": 3065}, {\"language\": \"Stoney\", \"mother_tongue\": 3025}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "barplot-mother-tongue-reorder"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue('barplot-mother-tongue-reorder', ordered_barplot_mother_tongue, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f504a0",
   "metadata": {},
   "source": [
    ":::{glue:figure} barplot-mother-tongue-reorder\n",
    ":figwidth: 700px\n",
    ":name: barplot-mother-tongue-reorder\n",
    "\n",
    "Bar plot of the ten Aboriginal languages most often reported by Canadian residents as their mother tongue with bars reordered.\n",
    ":::        \n",
    "\n",
    "\n",
    "{numref}`barplot-mother-tongue-reorder` provides a very clear and well-organized\n",
    "answer to our original question; we can see what the ten most often reported Aboriginal languages\n",
    "were, according to the 2016 Canadian census, and how many people speak each of them. For\n",
    "instance, we can see that the Aboriginal language most often reported was Cree\n",
    "n.o.s. with over 60,000 Canadian residents reporting it as their mother tongue.\n",
    "\n",
    "> **Note:** \"n.o.s.\" means \"not otherwise specified\", so Cree n.o.s. refers to\n",
    "> individuals who reported Cree as their mother tongue. In this data set, the\n",
    "> Cree languages include the following categories: Cree n.o.s., Swampy Cree,\n",
    "> Plains Cree, Woods Cree, and a 'Cree not included elsewhere' category (which\n",
    "> includes Moose Cree, Northern East Cree and Southern East Cree)\n",
    "> [@language2016]. \n",
    "\n",
    "### Putting it all together\n",
    "\n",
    "In the block of code below, we put everything from this chapter together, with a few\n",
    "modifications. In particular, we have actually skipped the\n",
    "`loc[]` step that we did above; since you specify the variable names to plot\n",
    "in the `altair` function, you don't actually need to select the columns in advance\n",
    "when creating a visualization. We have also provided *comments* next to \n",
    "many of the lines of code below using the\n",
    "hash symbol `#`. When Python sees a `#` sign, \\index{comment} \\index{aaacommentsymb@\\#|see{comment}} it \n",
    "will ignore all of the text that\n",
    "comes after the symbol on that line. So you can use comments to explain lines \n",
    "of code for others, and perhaps more importantly, your future self!\n",
    "It's good practice to get in the habit of\n",
    "commenting your code to improve its readability.\n",
    "\n",
    "This exercise demonstrates the power of Python. In relatively few lines of code, we\n",
    "performed an entire data science workflow with a highly effective data\n",
    "visualization! We asked a question, loaded the data into Python, wrangled the data\n",
    "(using `df[]`, `sort_values()` and `iloc[]`) and created a data visualization to\n",
    "help answer our question. In this chapter, you got a quick taste of the data\n",
    "science workflow; continue on with the next few chapters to learn each of \n",
    "these steps in much more detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "516a5ce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.722794Z",
     "iopub.status.busy": "2022-06-08T02:12:12.722655Z",
     "iopub.status.idle": "2022-06-08T02:12:12.727451Z",
     "shell.execute_reply": "2022-06-08T02:12:12.727122Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data set\n",
    "can_lang = pd.read_csv(\"data/can_lang.csv\")\n",
    "\n",
    "# obtain the 10 most common Aboriginal languages\n",
    "aboriginal_lang = can_lang[can_lang['category'] == 'Aboriginal languages']\n",
    "arranged_lang = selected_lang.sort_values(['mother_tongue'], ascending=False)\n",
    "ten_lang = arranged_lang[:10]\n",
    "\n",
    "# create the visualization\n",
    "ten_lang_plot = (alt.Chart(ten_lang)\n",
    "                 .mark_bar().encode(\n",
    "                  x = alt.X('mother_tongue', title=\"Mother Tongue (Number of Canadian Residents)\"),\n",
    "                  y = alt.Y('language', title=\"Language\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2562bf50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.729166Z",
     "iopub.status.busy": "2022-06-08T02:12:12.729050Z",
     "iopub.status.idle": "2022-06-08T02:12:12.733768Z",
     "shell.execute_reply": "2022-06-08T02:12:12.733474Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-2dde60e3e33d48f0839aa294f15c44f8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2dde60e3e33d48f0839aa294f15c44f8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2dde60e3e33d48f0839aa294f15c44f8\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0cdfcf8245507d928490a47b75d8fff0\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"mother_tongue\", \"title\": \"Mother Tongue (Number of Canadian Residents)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"language\", \"title\": \"Language\", \"type\": \"nominal\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-0cdfcf8245507d928490a47b75d8fff0\": [{\"language\": \"Cree, n.o.s.\", \"mother_tongue\": 64050}, {\"language\": \"Inuktitut\", \"mother_tongue\": 35210}, {\"language\": \"Ojibway\", \"mother_tongue\": 17885}, {\"language\": \"Oji-Cree\", \"mother_tongue\": 12855}, {\"language\": \"Dene\", \"mother_tongue\": 10700}, {\"language\": \"Montagnais (Innu)\", \"mother_tongue\": 10235}, {\"language\": \"Mi'kmaq\", \"mother_tongue\": 6690}, {\"language\": \"Atikamekw\", \"mother_tongue\": 6150}, {\"language\": \"Plains Cree\", \"mother_tongue\": 3065}, {\"language\": \"Stoney\", \"mother_tongue\": 3025}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "",
       "name": "final_plot"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "glue('final_plot', ten_lang_plot, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972b587",
   "metadata": {},
   "source": [
    ":::{glue:figure} final_plot\n",
    ":figwidth: 700px\n",
    ":name: final_plot\n",
    "\n",
    "Bar plot of the ten Aboriginal languages most often reported by Canadian residents as their mother tongue\n",
    ":::\n",
    "\n",
    "\n",
    "## Accessing documentation\n",
    "\n",
    "There are many Python functions in the `pandas` package (and beyond!), and \n",
    "nobody can be expected to remember what every one of them does\n",
    "or all of the arguments we have to give them. Fortunately, Python provides \n",
    "the `help` function and `__doc__` attrribute, which \n",
    "\\index{aaaquestionmark@?|see{documentation}}\n",
    "\\index{help|see{documentation}}\n",
    "\\index{documentation} provides an easy way to pull up the documentation for \n",
    "most functions quickly. To use the `help` function to access documentation, you \n",
    "just put the name of the function you are curious about as an argument inside the `help` function.\n",
    "For example, if you had forgotten what the `pd.read_csv` function\n",
    "did or exactly what arguments to pass in, you could run the following\n",
    "code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f30eb031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.735542Z",
     "iopub.status.busy": "2022-06-08T02:12:12.735384Z",
     "iopub.status.idle": "2022-06-08T02:12:12.738464Z",
     "shell.execute_reply": "2022-06-08T02:12:12.738174Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', sep=<no_default>, delimiter=None, header='infer', names=<no_default>, index_col=None, usecols=None, squeeze=None, prefix=<no_default>, mangle_dupe_cols=True, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=None, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression: 'CompressionOptions' = 'infer', thousands=None, decimal: 'str' = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors: 'str | None' = 'strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: 'StorageOptions' = None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, None, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, optional, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    \n",
      "        .. deprecated:: 1.4.0\n",
      "            Append ``.squeeze(\"columns\")`` to the call to ``read_csv`` to squeeze\n",
      "            the data.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    \n",
      "        .. deprecated:: 1.4.0\n",
      "           Use a list comprehension on the DataFrame's columns after calling ``read_csv``.\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "            The \"pyarrow\" engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and '%s' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', or '.zst' (otherwise no compression). If using\n",
      "        'zip', the ZIP file must contain only one data file to be read in. Set to\n",
      "        ``None`` for no decompression. Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``} and other\n",
      "        key-value pairs are forwarded to ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, or ``zstandard.ZstdDecompressor``, respectively. As an\n",
      "        example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "    \n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "    \n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
      "           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
      "           This behavior was previously only the case for ``engine=\"python\"``.\n",
      "    \n",
      "        .. versionchanged:: 1.3.0\n",
      "    \n",
      "           ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
      "           influence on how encoding errors are handled.\n",
      "    \n",
      "    encoding_errors : str, optional, default \"strict\"\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, optional, default ``None``\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will be dropped from the DataFrame that is\n",
      "        returned.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    warn_bad_lines : bool, optional, default ``None``\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "    \n",
      "            - 'error', raise an Exception when a bad line is encountered.\n",
      "            - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
      "            - 'skip', skip bad lines without raising or warning when they are encountered.\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "            - callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new list of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine=\"python\"``\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or 'high' for the ordinary converter,\n",
      "        'legacy' for the original lower precision pandas converter, and\n",
      "        'round_trip' for the round-trip converter.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "    \n",
      "        .. versionadded:: 1.2\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74829e2",
   "metadata": {},
   "source": [
    "To use the `__doc__` attribute to access documentation, you \n",
    "just put `.__doc__`  after the function you are curious about.\n",
    "For example, to access the documentation of `read_csv` you could run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92d0eb0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-08T02:12:12.740339Z",
     "iopub.status.busy": "2022-06-08T02:12:12.740223Z",
     "iopub.status.idle": "2022-06-08T02:12:12.742256Z",
     "shell.execute_reply": "2022-06-08T02:12:12.741910Z"
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "Also supports optionally iterating or breaking of the file\n",
      "into chunks.\n",
      "\n",
      "Additional help can be found in the online docs for\n",
      "`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "filepath_or_buffer : str, path object or file-like object\n",
      "    Any valid string path is acceptable. The string could be a URL. Valid\n",
      "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "sep : str, default ','\n",
      "    Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "    the separator, but the Python parsing engine can, meaning the latter will\n",
      "    be used and automatically detect the separator by Python's builtin sniffer\n",
      "    tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "    different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "    will also force the use of the Python parsing engine. Note that regex\n",
      "    delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "delimiter : str, default ``None``\n",
      "    Alias for sep.\n",
      "header : int, list of int, None, default 'infer'\n",
      "    Row number(s) to use as the column names, and the start of the\n",
      "    data.  Default behavior is to infer the column names: if no names\n",
      "    are passed the behavior is identical to ``header=0`` and column\n",
      "    names are inferred from the first line of the file, if column\n",
      "    names are passed explicitly then the behavior is identical to\n",
      "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "    replace existing names. The header can be a list of integers that\n",
      "    specify row locations for a multi-index on the columns\n",
      "    e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "    skipped (e.g. 2 in this example is skipped). Note that this\n",
      "    parameter ignores commented lines and empty lines if\n",
      "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "    data rather than the first line of the file.\n",
      "names : array-like, optional\n",
      "    List of column names to use. If the file contains a header row,\n",
      "    then you should explicitly pass ``header=0`` to override the column names.\n",
      "    Duplicates in this list are not allowed.\n",
      "index_col : int, str, sequence of int / str, or False, optional, default ``None``\n",
      "  Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "  string name or column index. If a sequence of int / str is given, a\n",
      "  MultiIndex is used.\n",
      "\n",
      "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "  column as the index, e.g. when you have a malformed file with delimiters at\n",
      "  the end of each line.\n",
      "usecols : list-like or callable, optional\n",
      "    Return a subset of the columns. If list-like, all elements must either\n",
      "    be positional (i.e. integer indices into the document columns) or strings\n",
      "    that correspond to column names provided either by the user in `names` or\n",
      "    inferred from the document header row(s). If ``names`` are given, the document\n",
      "    header row(s) are not taken into account. For example, a valid list-like\n",
      "    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "    To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "    in ``['foo', 'bar']`` order or\n",
      "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "    for ``['bar', 'foo']`` order.\n",
      "\n",
      "    If callable, the callable function will be evaluated against the column\n",
      "    names, returning names where the callable function evaluates to True. An\n",
      "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "    parsing time and lower memory usage.\n",
      "squeeze : bool, default False\n",
      "    If the parsed data only contains one column then return a Series.\n",
      "\n",
      "    .. deprecated:: 1.4.0\n",
      "        Append ``.squeeze(\"columns\")`` to the call to ``read_csv`` to squeeze\n",
      "        the data.\n",
      "prefix : str, optional\n",
      "    Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "\n",
      "    .. deprecated:: 1.4.0\n",
      "       Use a list comprehension on the DataFrame's columns after calling ``read_csv``.\n",
      "mangle_dupe_cols : bool, default True\n",
      "    Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "    'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "    are duplicate names in the columns.\n",
      "dtype : Type name or dict of column -> type, optional\n",
      "    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "    'c': 'Int64'}\n",
      "    Use `str` or `object` together with suitable `na_values` settings\n",
      "    to preserve and not interpret dtype.\n",
      "    If converters are specified, they will be applied INSTEAD\n",
      "    of dtype conversion.\n",
      "engine : {'c', 'python', 'pyarrow'}, optional\n",
      "    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "    is currently more feature-complete. Multithreading is currently only supported by\n",
      "    the pyarrow engine.\n",
      "\n",
      "    .. versionadded:: 1.4.0\n",
      "\n",
      "        The \"pyarrow\" engine was added as an *experimental* engine, and some features\n",
      "        are unsupported, or may not work correctly, with this engine.\n",
      "converters : dict, optional\n",
      "    Dict of functions for converting values in certain columns. Keys can either\n",
      "    be integers or column labels.\n",
      "true_values : list, optional\n",
      "    Values to consider as True.\n",
      "false_values : list, optional\n",
      "    Values to consider as False.\n",
      "skipinitialspace : bool, default False\n",
      "    Skip spaces after delimiter.\n",
      "skiprows : list-like, int or callable, optional\n",
      "    Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "    at the start of the file.\n",
      "\n",
      "    If callable, the callable function will be evaluated against the row\n",
      "    indices, returning True if the row should be skipped and False otherwise.\n",
      "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "skipfooter : int, default 0\n",
      "    Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "nrows : int, optional\n",
      "    Number of rows of file to read. Useful for reading pieces of large files.\n",
      "na_values : scalar, str, list-like, or dict, optional\n",
      "    Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "    per-column NA values.  By default the following values are interpreted as\n",
      "    NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "    'nan', 'null'.\n",
      "keep_default_na : bool, default True\n",
      "    Whether or not to include the default NaN values when parsing the data.\n",
      "    Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "\n",
      "    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "      is appended to the default NaN values used for parsing.\n",
      "    * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "      the default NaN values are used for parsing.\n",
      "    * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "      the NaN values specified `na_values` are used for parsing.\n",
      "    * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "      strings will be parsed as NaN.\n",
      "\n",
      "    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "    `na_values` parameters will be ignored.\n",
      "na_filter : bool, default True\n",
      "    Detect missing value markers (empty strings and the value of na_values). In\n",
      "    data without any NAs, passing na_filter=False can improve the performance\n",
      "    of reading a large file.\n",
      "verbose : bool, default False\n",
      "    Indicate number of NA values placed in non-numeric columns.\n",
      "skip_blank_lines : bool, default True\n",
      "    If True, skip over blank lines rather than interpreting as NaN values.\n",
      "parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "    The behavior is as follows:\n",
      "\n",
      "    * boolean. If True -> try parsing the index.\n",
      "    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "      each as a separate date column.\n",
      "    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "      a single date column.\n",
      "    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "      result 'foo'\n",
      "\n",
      "    If a column or index cannot be represented as an array of datetimes,\n",
      "    say because of an unparsable value or a mixture of timezones, the column\n",
      "    or index will be returned unaltered as an object data type. For\n",
      "    non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "    ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "    specify ``date_parser`` to be a partially-applied\n",
      "    :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "    :ref:`io.csv.mixed_timezones` for more.\n",
      "\n",
      "    Note: A fast-path exists for iso8601-formatted dates.\n",
      "infer_datetime_format : bool, default False\n",
      "    If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "    format of the datetime strings in the columns, and if it can be inferred,\n",
      "    switch to a faster method of parsing them. In some cases this can increase\n",
      "    the parsing speed by 5-10x.\n",
      "keep_date_col : bool, default False\n",
      "    If True and `parse_dates` specifies combining multiple columns then\n",
      "    keep the original columns.\n",
      "date_parser : function, optional\n",
      "    Function to use for converting a sequence of string columns to an array of\n",
      "    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "    conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "    string values from the columns defined by `parse_dates` into a single array\n",
      "    and pass that; and 3) call `date_parser` once for each row using one or\n",
      "    more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "    arguments.\n",
      "dayfirst : bool, default False\n",
      "    DD/MM format dates, international and European format.\n",
      "cache_dates : bool, default True\n",
      "    If True, use a cache of unique, converted dates to apply the datetime\n",
      "    conversion. May produce significant speed-up when parsing duplicate\n",
      "    date strings, especially ones with timezone offsets.\n",
      "\n",
      "    .. versionadded:: 0.25.0\n",
      "iterator : bool, default False\n",
      "    Return TextFileReader object for iteration or getting chunks with\n",
      "    ``get_chunk()``.\n",
      "\n",
      "    .. versionchanged:: 1.2\n",
      "\n",
      "       ``TextFileReader`` is a context manager.\n",
      "chunksize : int, optional\n",
      "    Return TextFileReader object for iteration.\n",
      "    See the `IO Tools docs\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "    for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "    .. versionchanged:: 1.2\n",
      "\n",
      "       ``TextFileReader`` is a context manager.\n",
      "compression : str or dict, default 'infer'\n",
      "    For on-the-fly decompression of on-disk data. If 'infer' and '%s' is\n",
      "    path-like, then detect compression from the following extensions: '.gz',\n",
      "    '.bz2', '.zip', '.xz', or '.zst' (otherwise no compression). If using\n",
      "    'zip', the ZIP file must contain only one data file to be read in. Set to\n",
      "    ``None`` for no decompression. Can also be a dict with key ``'method'`` set\n",
      "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``} and other\n",
      "    key-value pairs are forwarded to ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "    ``bz2.BZ2File``, or ``zstandard.ZstdDecompressor``, respectively. As an\n",
      "    example, the following could be passed for Zstandard decompression using a\n",
      "    custom compression dictionary:\n",
      "    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "\n",
      "    .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "thousands : str, optional\n",
      "    Thousands separator.\n",
      "decimal : str, default '.'\n",
      "    Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "lineterminator : str (length 1), optional\n",
      "    Character to break file into lines. Only valid with C parser.\n",
      "quotechar : str (length 1), optional\n",
      "    The character used to denote the start and end of a quoted item. Quoted\n",
      "    items can include the delimiter and it will be ignored.\n",
      "quoting : int or csv.QUOTE_* instance, default 0\n",
      "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "doublequote : bool, default ``True``\n",
      "   When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "   whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "   field as a single ``quotechar`` element.\n",
      "escapechar : str (length 1), optional\n",
      "    One-character string used to escape other characters.\n",
      "comment : str, optional\n",
      "    Indicates remainder of line should not be parsed. If found at the beginning\n",
      "    of a line, the line will be ignored altogether. This parameter must be a\n",
      "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "    fully commented lines are ignored by the parameter `header` but not by\n",
      "    `skiprows`. For example, if ``comment='#'``, parsing\n",
      "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "    treated as the header.\n",
      "encoding : str, optional\n",
      "    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "    standard encodings\n",
      "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "    .. versionchanged:: 1.2\n",
      "\n",
      "       When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
      "       ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
      "       This behavior was previously only the case for ``engine=\"python\"``.\n",
      "\n",
      "    .. versionchanged:: 1.3.0\n",
      "\n",
      "       ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
      "       influence on how encoding errors are handled.\n",
      "\n",
      "encoding_errors : str, optional, default \"strict\"\n",
      "    How encoding errors are treated. `List of possible values\n",
      "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "    .. versionadded:: 1.3.0\n",
      "\n",
      "dialect : str or csv.Dialect, optional\n",
      "    If provided, this parameter will override values (default or not) for the\n",
      "    following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "    override values, a ParserWarning will be issued. See csv.Dialect\n",
      "    documentation for more details.\n",
      "error_bad_lines : bool, optional, default ``None``\n",
      "    Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "    default cause an exception to be raised, and no DataFrame will be returned.\n",
      "    If False, then these \"bad lines\" will be dropped from the DataFrame that is\n",
      "    returned.\n",
      "\n",
      "    .. deprecated:: 1.3.0\n",
      "       The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "       encountering a bad line instead.\n",
      "warn_bad_lines : bool, optional, default ``None``\n",
      "    If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "    \"bad line\" will be output.\n",
      "\n",
      "    .. deprecated:: 1.3.0\n",
      "       The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "       encountering a bad line instead.\n",
      "on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n",
      "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "    Allowed values are :\n",
      "\n",
      "        - 'error', raise an Exception when a bad line is encountered.\n",
      "        - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
      "        - 'skip', skip bad lines without raising or warning when they are encountered.\n",
      "\n",
      "    .. versionadded:: 1.3.0\n",
      "\n",
      "        - callable, function with signature\n",
      "          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "          If the function returns ``None``, the bad line will be ignored.\n",
      "          If the function returns a new list of strings with more elements than\n",
      "          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "          Only supported when ``engine=\"python\"``\n",
      "\n",
      "    .. versionadded:: 1.4.0\n",
      "\n",
      "delim_whitespace : bool, default False\n",
      "    Specifies whether or not whitespace (e.g. ``' '`` or ``'\t'``) will be\n",
      "    used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "    is set to True, nothing should be passed in for the ``delimiter``\n",
      "    parameter.\n",
      "low_memory : bool, default True\n",
      "    Internally process the file in chunks, resulting in lower memory use\n",
      "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "    types either set False, or specify the type with the `dtype` parameter.\n",
      "    Note that the entire file is read into a single DataFrame regardless,\n",
      "    use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "    (Only valid with C parser).\n",
      "memory_map : bool, default False\n",
      "    If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "    directly onto memory and access the data directly from there. Using this\n",
      "    option can improve performance because there is no longer any I/O overhead.\n",
      "float_precision : str, optional\n",
      "    Specifies which converter the C engine should use for floating-point\n",
      "    values. The options are ``None`` or 'high' for the ordinary converter,\n",
      "    'legacy' for the original lower precision pandas converter, and\n",
      "    'round_trip' for the round-trip converter.\n",
      "\n",
      "    .. versionchanged:: 1.2\n",
      "\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "    starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "    ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "\n",
      "    .. versionadded:: 1.2\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame or TextParser\n",
      "    A comma-separated values (csv) file is returned as two-dimensional\n",
      "    data structure with labeled axes.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_csv.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad0eea",
   "metadata": {},
   "source": [
    "{numref}`help_read_csv` shows the documentation that will pop up,\n",
    "including a high-level description of the function, its arguments, \n",
    "a description of each, and more. Note that you may find some of the\n",
    "text in the documentation a bit too technical right now \n",
    "Fear not: as you work through this book, many of these terms will be introduced\n",
    "to you, and slowly but surely you will become more adept at understanding and navigating \n",
    "documentation like that shown in {numref}`help_read_csv`. But do keep in mind that the documentation\n",
    "is not written to *teach* you about a function; it is just there as a reference to *remind*\n",
    "you about the different arguments and usage of functions that you have already learned about elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cf487",
   "metadata": {},
   "source": [
    "```{figure} img/help_read_csv.png\n",
    "---\n",
    "height: 700px\n",
    "name: help_read_csv\n",
    "---\n",
    "The documentation for the read_csv function including a high-level description, a list of arguments and their meanings, and more.\n",
    "```\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Practice exercises for the material covered in this chapter \n",
    "can be found in the accompanying \n",
    "[worksheets repository](https://github.com/UBC-DSCI/data-science-a-first-intro-worksheets#readme)\n",
    "in the \"R and the tidyverse\" row.\n",
    "You can launch an interactive version of the worksheet in your browser by clicking the \"launch binder\" button.\n",
    "You can also preview a non-interactive version of the worksheet by clicking \"view worksheet.\"\n",
    "If you instead decide to download the worksheet and run it on your own machine,\n",
    "make sure to follow the instructions for computer setup\n",
    "found in Chapter \\@ref(move-to-your-own-machine). This will ensure that the automated feedback\n",
    "and guidance that the worksheets provide will function as intended."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,md:myst,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
